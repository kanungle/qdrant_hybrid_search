{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e5f1a5c2",
      "metadata": {
        "id": "e5f1a5c2"
      },
      "source": [
        "# Hybrid Search with Reranking and Filtering\n",
        "_Last updated: June 1, 2025_\n",
        "\n",
        "This notebook demonstrates advanced hybrid search techniques combining three different [embedding](https://en.wikipedia.org/wiki/Word_embedding) approaches: **dense**, **sparse**, and **late interaction**. It uses dense and sparse embeddings for initial retrieval, then late interaction for reranking. It also filters by `user_id` to simulate multitenancy.\n",
        "\n",
        "### What is Hybrid Search?\n",
        "Hybrid search combines multiple retrieval methods to improve search quality and relevance. Instead of relying on a single approach, we leverage the strengths of different embedding techniques:\n",
        "\n",
        "- **Dense Embeddings** (Semantic Search): Uses neural networks to create dense vector representations of unstructured data, capturing semantic meaning and context. Great for finding conceptually similar content.\n",
        "\n",
        "- **Sparse Embeddings** (Keyword Search): Uses keyword-based search through BM25 for finding exact matches in words. It's great for maintaining interpretability and precision, especially in use cases where industry-specific terms are used.\n",
        "\n",
        "- **Late Interaction Embeddings** (Advanced Semantic): Allows each [token](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation) to get it's own embedding, enabling precise matching and fine-grained interactions. It combines the benefits of dense retrieval and token-level precision, making it great for reranking in this workflow.\n",
        "\n",
        "**Why?** By combining these three approaches, we get more robustness to different types of queries to the vector database\n",
        "\n",
        "### References and Resources\n",
        "The following were used to complete this project:\n",
        "- [Qdrant Reranking in Hybrid Search](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/?q=ingest#ingestion-stage)\n",
        "- [Qdrant 'Concepts' Documentation](https://qdrant.tech/documentation/concepts/)\n",
        "- [How to Build the Ultimate Hybrid Search with Qdrant (video)](https://www.youtube.com/live/LAZOxqzceEU?si=4HF34v9G1xq3Z3-6)\n",
        "- [Anthropic's Claude](https://claude.ai/new) (for coding support like troubleshooting, debugging, cleanup)\n",
        "- [Hugging Face Datasets](https://huggingface.co/datasets?modality=modality:text&sort=trending) (for source documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "323a8135",
      "metadata": {
        "id": "323a8135"
      },
      "source": [
        "## Environment Setup\n",
        "\n",
        "First we install all the required libraries for this notebook:\n",
        "\n",
        "- **qdrant-client**: Qdrant's vector database client for storing and retrieving embeddings\n",
        "- **fastembed**: Qdrant's opensource, lightweight, and comprehensive library for generating different embedding types\n",
        "- **fastembed-gpu**: The version of fastembed that utilizes GPU acceleration (when available)\n",
        "- **tqdm**: Visual progress bars to track long-running operations\n",
        "- **polars**: Fast dataframe library for data manipulation\n",
        "- **torch**: PyTorch for GPU acceleration (when available)\n",
        "- **rerankers**: Comprehensive library of rerankers, including ColBERT\n",
        "\n",
        "We then import all the libraries and modules we need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "3922b9bd",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3922b9bd",
        "outputId": "5fd218d4-400e-42fd-fa97-9e9a636642ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting qdrant-client\n",
            "  Downloading qdrant_client-1.14.2-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fastembed\n",
            "  Downloading fastembed-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fastembed-gpu\n",
            "  Downloading fastembed_gpu-0.7.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.11/dist-packages (1.21.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting rerankers\n",
            "  Downloading rerankers-0.10.0-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (1.71.0)\n",
            "Requirement already satisfied: httpx>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.21 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.0.2)\n",
            "Collecting portalocker<3.0.0,>=2.7.0 (from qdrant-client)\n",
            "  Downloading portalocker-2.10.1-py3-none-any.whl.metadata (8.5 kB)\n",
            "Requirement already satisfied: protobuf>=3.20.0 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (5.29.4)\n",
            "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.11.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/lib/python3.11/dist-packages (from qdrant-client) (2.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in /usr/local/lib/python3.11/dist-packages (from fastembed) (0.31.4)\n",
            "Collecting loguru<0.8.0,>=0.7.2 (from fastembed)\n",
            "  Downloading loguru-0.7.3-py3-none-any.whl.metadata (22 kB)\n",
            "Collecting mmh3<6.0.0,>=4.1.0 (from fastembed)\n",
            "  Downloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (16 kB)\n",
            "Collecting onnxruntime!=1.20.0,>=1.17.0 (from fastembed)\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: pillow<12.0.0,>=10.3.0 in /usr/local/lib/python3.11/dist-packages (from fastembed) (11.2.1)\n",
            "Collecting py-rust-stemmers<0.2.0,>=0.1.0 (from fastembed)\n",
            "  Downloading py_rust_stemmers-0.1.5-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: requests<3.0,>=2.31 in /usr/local/lib/python3.11/dist-packages (from fastembed) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1.0,>=0.15 in /usr/local/lib/python3.11/dist-packages (from fastembed) (0.21.1)\n",
            "Collecting onnxruntime-gpu!=1.20.0,>=1.17.0 (from fastembed-gpu)\n",
            "  Downloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.20->fastembed) (6.0.2)\n",
            "Collecting coloredlogs (from onnxruntime!=1.20.0,>=1.17.0->fastembed)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime!=1.20.0,>=1.17.0->fastembed) (25.2.10)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.31->fastembed) (3.4.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime!=1.20.0,>=1.17.0->fastembed)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Downloading qdrant_client-1.14.2-py3-none-any.whl (327 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m327.7/327.7 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastembed-0.7.0-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m99.0/99.0 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastembed_gpu-0.7.0-py3-none-any.whl (99 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m116.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m101.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rerankers-0.10.0-py3-none-any.whl (53 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading loguru-0.7.3-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m61.6/61.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mmh3-5.1.0-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (101 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m112.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime_gpu-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (283.2 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m283.2/283.2 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading portalocker-2.10.1-py3-none-any.whl (18 kB)\n",
            "Downloading py_rust_stemmers-0.1.5-cp311-cp311-manylinux_2_28_x86_64.whl (324 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m324.8/324.8 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: py-rust-stemmers, rerankers, portalocker, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, mmh3, loguru, humanfriendly, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, onnxruntime-gpu, onnxruntime, nvidia-cusolver-cu12, qdrant-client, fastembed-gpu, fastembed\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed coloredlogs-15.0.1 fastembed-0.7.0 fastembed-gpu-0.7.0 humanfriendly-10.0 loguru-0.7.3 mmh3-5.1.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnxruntime-1.22.0 onnxruntime-gpu-1.22.0 portalocker-2.10.1 py-rust-stemmers-0.1.5 qdrant-client-1.14.2 rerankers-0.10.0\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "%pip install qdrant-client fastembed fastembed-gpu tqdm polars torch rerankers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "49c2cd1a",
      "metadata": {
        "id": "49c2cd1a"
      },
      "outputs": [],
      "source": [
        "# Import dependencies\n",
        "from qdrant_client import QdrantClient, models\n",
        "from fastembed import TextEmbedding, LateInteractionTextEmbedding, SparseTextEmbedding\n",
        "import polars as pl\n",
        "from tqdm import tqdm\n",
        "from getpass import getpass\n",
        "import random, os\n",
        "import torch\n",
        "from rerankers import Reranker"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7a3e6f4",
      "metadata": {
        "id": "b7a3e6f4"
      },
      "source": [
        "## Create Embeddings\n",
        "\n",
        "This section covers the creation of two different types of embeddings needed for hybrid search. Each embedding type captures different aspects of the source text:\n",
        "\n",
        "- **Dense embeddings**: Fixed-size vectors, mostly non-zero, capturing semantic meaning\n",
        "- **Sparse embeddings**: Variable-size vectors, mostly zero, capturing keyword term-frequency information."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86f9ef5e",
      "metadata": {
        "id": "86f9ef5e"
      },
      "source": [
        "### Embedding Setup\n",
        "\n",
        "We must first initialize our embedding models, then we can load our document dataset to start embedding. The source documents for this project were **arxiv paper abstracts**, found on [Hugging Face](https://huggingface.co/datasets/bluuebunny/arxiv_abstract_embedding_mxbai_large_v1_milvus_binary) and published by Mitanshu Sukhwani in 2025. This dataset provides a rich corpus of scientific text for demonstrating text retrieval. We random sample 1 million abstracts from this dataset to reduce overall embeddings to the minimum required volume."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "e937f1e4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 941,
          "referenced_widgets": [
            "8ac288c3fcd24dd4a64aa916a57793e6",
            "46252d12f7134c9e9045116cbc581e21",
            "ad130c36f4c34dc7b81623f694436a01",
            "809c5ff9143c4401b22263bfb6a72670",
            "be63878682984980a69e9b957283d608",
            "10e75d08370345f7b1ea0ee7934e3746",
            "09ea4c25e0794231a2d1d7c6ddf8fe17",
            "3743030fb4f04b3182304afa57d1fca7",
            "fee26ecbad96446381bdfe87211f7643",
            "729a2d1dcaa84edeafb275e7cf779af8",
            "a87f2c7bd2c64dbe8ab559f1b7ecd7ce",
            "d1510e3863ef4cf4aa706ccfa5020753",
            "2714e85c46ed423d889adae13f3b4602",
            "83c5dcf3746f456c9f0354afee3430be",
            "58b6082fa89d45bbbda6371c23b8f658",
            "4b7dd783cdf744a58c9dd697b0f066f2",
            "570baf47693747dcaea51d97da088d73",
            "e3f0c0319b3e4038beb922116a5cf3be",
            "4693fd8b546e43748d88f5a82b6cde33",
            "8d43d0eeecda45b78165846e6ed5d460",
            "031b62b4ec3c412d82ce8d36a6ffa7d7",
            "46d09eff976c45bbb9dc93d83f71fe5f",
            "ab18b50d867349dbbabc090d5a6bcb22",
            "5cb526cd62d347bc94fc71a2803cf9ae",
            "b68db64a84da4342ad07248aa04d112c",
            "eb9a80c870bc4a7dace49393ec582791",
            "169dfa44248a4778b24f16f3d7bc384f",
            "ac3c099250994fca9b0a557b9be7e034",
            "cf14a045f9dc49d78c64ca64b79686bd",
            "24f7fb8facb54869816b171f81172a90",
            "e753c6dc82ed4a16b239d7a9877cabcc",
            "a6b7b495d8a44f64ba6008b3426b5ab7",
            "46836b40d5db4a74ae4f9f40d66ea7f8",
            "bc4c15e952ce40e08941a2e271b05c27",
            "c36e6dea899d49aea84d3acfd59f5e86",
            "cb508884bb7a4ad684b5dcd2b4f0fd05",
            "2c5f6e6e63004a44a5dfb702f9bd18f4",
            "29452c399dea4b6baa9527ebcade402c",
            "d8e5e3df52e74f6cbe9e0dc732950fba",
            "a4cdde30f21743c7939bf13886fd28f3",
            "bbe8b6c2dd7f4c7b95ee7bbba6f4c43e",
            "17542fe10fb64c55854ac88034a8be3f",
            "8362ed4525394f4ea55c7b6371792299",
            "fc15977c77084b3aaa515b5f2131a256",
            "ef32f4bbf7a84c9db92dc2629dcd2cde",
            "46e45cf5d9d64d84a7cca45a0fab9eec",
            "b9840681ca524f01be27f3b44f37ae94",
            "b6db9e897c984c50b89ae5e1421954e2",
            "4b3133190517443da429e181c7bb8b53",
            "e5a3cfbfdc254aa1bf42c0eb9f83809c",
            "b7c0c1381394469c9ab0838566beecb7",
            "18fb332c562e44ba984fdd4bf7915eaf",
            "fb5f5fca15fb4719a772aac129b873ad",
            "e37e6a7783814d4aa382f8142c4e8734",
            "ffa863b83ab844b6b0b6acc04a71bd79",
            "a5380943f94340fe96ee4f87adf86aaa",
            "daf3add14a3745139437c45bceb58a3d",
            "25ba8f7e050140deb8ff4d68d492ab06",
            "ec12e7f529cf4de0a2a865555db3661c",
            "5ac51ff77b004be48b76322fc08d1e64",
            "413d3a8b9b634663acc7559a72af112e",
            "60976e65250f4d66b3b0d8171a0ec70f",
            "c2bb94670b454f74b7d3fe77b4e18cb2",
            "ab093227a34749d7a20de499ea48c29b",
            "8379809739504feaba07464492184c41",
            "c58d7989788f41299a3a3103b46c59dc",
            "2dd1f435bb6842748908b330b52a2ef3",
            "72d9e068037947dfa7d674d5afecfdfa",
            "523af145584b4547a38e47a5867d83d8",
            "f28561591fcf4913a74dfca9cd1ea870",
            "5364b1818da4457790f5d6e0076d753d",
            "d5210d35b0ea4dec92c0d06f19cede52",
            "80694d14aaaa40b9b17499adb1b3bae6",
            "ff317c8a94f64a63bbc194ed2c68e312",
            "e0d3584a5c644ec491d3062951101d87",
            "665e8211f72f40088bf506988f7482c5",
            "1ba10db11dc8488da19982a5bcec7fef",
            "c55245cc4a49476ebc353f5d54734df5",
            "c88ad9109e9d434b801ccdee7ed651c2",
            "2e7014f09bda48618a847c329c881b02",
            "57c52034cffe49a0b8e3c51ff1dc0987",
            "8249129eff1846d8a2ca7c7a193d35cb",
            "d821e965f1de47fa9000311da1b1d84d",
            "b48cb7149fd8452b87721073d737d55b",
            "42652d85db3e4549ad201651f5d71f5c",
            "b9eb551d5ad7438aa71d3a70a9cab4ac",
            "37f5598f4d8848a6a66d827cab87f661",
            "bfc90f9c0cc54ebe97f20dc5998fbca1",
            "e162d40ff131449d841504417f8aba08",
            "51e1eb491d0e4bd7a5d0af781f299aaa",
            "16392f5b733b43119254b005f3c11d3b",
            "52ba570724f44ebeaed2d99113afb79f",
            "e0a4a182ca1e49bf9dc2a6f35d8ed87b",
            "f3f56d61c6d74a4cad02983c0c7488c0",
            "699e4b46223b41c0851f744347666c95",
            "0b80b148cf1648c7bfbcf92c22fe3fd2",
            "66f0ef0ec2994e6cbec134fb51fb8bee",
            "d5dbdddb0bb74d17942688b84f611657",
            "bce2d05418c2417c9d44a394c64f092a",
            "b83b2b86b1d94176b1f6c4f567818396",
            "e8bc0799282745c384377c7aa6ee88c1",
            "0a7582cd8bf34c43b769547ed93f9993",
            "4cbbf451e8e143ffa9b45491ea313f78",
            "662db9dbc3de4000844688455313ecb8",
            "42e6f7ce65724ebd8f1438ef5a7eda9a",
            "65b03ec1d3f24797828af3a3334188ce",
            "99cb47f90e8748d490b3a9e0c8e6e885",
            "6b32fea7305241369d46f807415df12a",
            "74eea3a4200249afb36f4d7b59bbb113",
            "4dd4bb1f0f404e05b4bfa863811e06c8",
            "a07459ce1aa74dd78bd795b690434d8f",
            "1e8132a843284f43961195d8ed6dbae3",
            "9e613f964b0c4ca5b90ef8667bba31fd",
            "7901c001266443af8b3c4b9d525344aa",
            "ae035fc1bfee450ebc5f9e4f4c6eba46",
            "2e5cdfc164624671b62b53feb9728515",
            "e8ed9d52195f4d5cac17371dc4aeb982",
            "f5520b7a01d041f9924b4906287f0114",
            "e397177fa0864b13a3a0b9a1f1a0ace1",
            "2bcb366c2a7e4d8182a0e37ee82c58b1",
            "29250cacb434408a8d43f72b5f57be3d",
            "b2d862fea2a846eaa5bd69adf9687bd0",
            "507e3484072143c4b481de414403762d",
            "8ac9e136e12e4da0b00099fa0156e9e1",
            "52101d61dea04945a79eba35db4428b6",
            "2954316cfb914ca8aba7eff80bed898a",
            "5364350ffd5d4404bf1c2c702268c361",
            "9363bbbff378468cbbc48db1a1c0f7ac",
            "b3bf4060d560480a9dde26eead261383",
            "b2e80022cad14a6eb1774151dad81522",
            "38f2a2d09ea14e34abf27908b8befffd",
            "66d9b90508d64101b371337c060cbd66",
            "a903506a618645568ae6f46258d03e3e",
            "06992a2f8fde486fa0b2326157605d7d",
            "f6b34f1544b848f681139f4a755c2f25",
            "998d33cae95748cf884000829730c7c5",
            "8ed31e3261d44fb8812cd458406606ac",
            "2dc36b89930a42269c0f2acf146f8b2a",
            "60093a1e10f643aeb8a3960297fbeb2b",
            "96819bce2e4d4d29b793f25affadd3ec",
            "0e842eae451c4dc4ba2a2e5bca3c3e45",
            "f00c349726584f848adf2962f4469cf8",
            "bde46f5e3369474aa4af366e318b57dd",
            "ca4d854a437c43cd9879598e2d231564",
            "d1ea4201b9584266a4e6282dbbfdb224",
            "b059ddf12514472792012e196a7b0962",
            "7460cf846e9d4ccfb29125723ff1bb61",
            "7f3f51a41aea4277ba1c6010036a7aa4",
            "6928ce9c90584dab8a8a9d134c5410c2",
            "e1898ecb6a6c4f469e219d97e9f3a655",
            "67d9db6afb5e46528f3d70f6e929d5eb",
            "1eed7bbbad2c48c5b9047f1e605e0e12",
            "9462cb6279754ea0bd67cc96e683c60d",
            "327f2d31cbbc49bd800f7eeaf4c4140b",
            "ac688c715e14400c8707696411fe9b09",
            "7de2530735604a5db57bb363e2938b56",
            "ce77c992024e4a379c7e845cd1a507b9",
            "47cd5e05283e44909bd3e86b9628d1d1",
            "589fb15095b644bdb9bff474e30739bc",
            "60cefe9c6c4b4ca2baa5100b54792c14",
            "759c7f70e2fb4007876705f65f5b074a",
            "2f896d56fd6f4cf58448c7f87656beb3",
            "02485d0c71f94353bc331e4dd994e710",
            "e0a0e47148b14221af3b7aaa297d13c0",
            "84f08ddae8d9434c89ddc4699395d870",
            "d374e796ab0443379350179a886b4f8c",
            "6e926b6b58a3479face818c653b391c8",
            "ca61f66538134609acd777e8486d135b",
            "7841348245724e46a3bb74bf95231a2b",
            "0b4450900f6c422498a0c68ba3bc623a",
            "e58a2779121242e2973987aa4990974a",
            "5d94765208a446238845b9d9b1ca96fb",
            "90b8788f9cbc47048598dbce9068f38f",
            "7519d6002d1c43cbb4b02e30c95b4b27",
            "84d55636288a49848cff5508f70564df",
            "76b3db04e6314d58b58099a578af075e",
            "b76e4c731f5542e8b0b858720ca8fc73",
            "03555d74b2e640e8b54b8553e233d463",
            "39de5489839d40f483cb8ee51ab0edab",
            "fd42788404f34d76b5074ca57ad6b05c",
            "d7f8e2889aaa4c43a525abca44229482",
            "f24145195026475395670486d92e3111",
            "d36af8ed57cd4737aea1fa8aa692bfa1",
            "70888305a5324ab0a499169506a2fa3e",
            "e59796791d7f4c31b75a849346969dea",
            "8dcadb5acc5d4d7e873013ef83a0a84a",
            "c1462d2c9cfa4babb98c05957365a34c",
            "237dc598580b489caeb5dc7f74bcbd1d",
            "5d386c4c87ed41a59cbff3b75419e660",
            "56a24755a14545c787765b610c231ba0",
            "ce2a5cf6d4324c33bc2e0d57f366f8e5",
            "9a60954e74b9424ca37f8a9c491766ae",
            "d3a366b3e0f24ddf90e8fc964b1ca057",
            "634731a3fad046b6b68bc73723b3b1d1",
            "5220cdaa4111463ea43c3bd94d702058",
            "90c767a2acfc4e53a324826e5f3cc25e",
            "92330bc0d25f4e929fad848d353dbd79",
            "a708be3a68bc4d77ba11b8c023a01b72",
            "5053ac51fead4c39989b9c6f9ddf628b",
            "24ff588424af4adb9d7679018c7d622f",
            "c6ab4a98db884f50acea0bf967025468",
            "3e315077e16d45c088aabeddb907361b",
            "c0baf7a343184464bea0af16a2f7c380",
            "40c4cd76ada2420cb4795ae1958053ff",
            "c1012004190c47ce9fb562c469493a1b",
            "772e3ab09fc542dab4cfbadf8e07f3ef",
            "c8c53bd2ea4d4567ab9cc44008f5f490",
            "deb3bd26fdb14cbba2312694e7fe2dda",
            "763f166a257a4c6d9595895b889e5737",
            "b44027596789489ba1639c79c54d3043",
            "a1edc334824441cf847bfc4f4dc3b410",
            "aa877187b4c34a65831ab5e193f60635",
            "b92f3e85aeda479db575abee34bbed5d",
            "e341ff2aab8a47ba9fd6987138323026",
            "a4d327f94bcd40bab99d33efc33823f7",
            "185d69cf0b8740b888ff96fac8947b94",
            "fefd91d2db51478e8346269711d28ba9",
            "dd9530c4694b4968b377ad365335ee51",
            "5438bceea62a4541b675f52abb115987",
            "444f769ccc2f4c93ae4a7e9edb7d8fb8",
            "7c1bbb4a08b34ac88d47488a07570cc4",
            "8c32f90eece24e8a901153357aca97fb",
            "ef9cf10698704149b2248ca50c976b8c",
            "0ed0d43306fb47f7881782df8aff8d1c",
            "457193586df04dbbb924a2809c81672d",
            "d8e44d9c84034e648630d7b41b96cae3",
            "4f9c81d274a541029caaacce7183b821",
            "3065cabc9fe248c1abf21baaa2d59322",
            "ed9bb7cd65ef42799783f0ec2d07f944",
            "0c0cec1c905e45fba68b105d1daa5ab0",
            "07a4cb9d384743a0a1fa37fd8aa04e30",
            "50970dbe554b4379abbcb0029a34b9b3",
            "4553a30c534841219989f134c08542b7",
            "0decfe844d94415994c711d6ae3039d6",
            "240d8a63294a45c5b9cd043800c23bb0",
            "fc2c3ccda56943e6ba3b11ae260e43cf",
            "92718858cb174e4b9d6ed33aa4da6982",
            "9002126828c54b1a87579f06de646ae7",
            "235a4d58640d406db9a8b0ddcd9fe1f9",
            "296a8e32c77e4675a341e6e09e333624",
            "8e9f33cd5105476db15410106407dca6",
            "845e48f40de24dfea4dbac99092e69f0",
            "a97b4dc679b34bd183d28e18131277f5",
            "243046ce5801476f95b9d73710e9b918",
            "a592643a05944a31a286ab70297d0c44",
            "b9c7a1d83e644daa99a31f0fe6fe5d77",
            "bffad762c9114125abeda498778be0c3",
            "211dac68e842486fba3c7f6ce982389e",
            "62f9a2e2606f4a0ea87fb30ff6295e1c",
            "710d6d7e565e42daa3db90b71ddde100",
            "f2e080f0ff0b4d37b4bcb06cf0d41111",
            "10be79c9df0e4d159da35bf06899346d",
            "5263b131cfaa448e8fb42d63951963c5",
            "09f956902b9c4ac7a35ab050e97fb54e",
            "6e174f7fc48d4d62ad7143ea91a29368",
            "172e30e1c5ba4d9db36bd61608d47f2a",
            "b86e42a00d274f9292ee16b21b3eeb29",
            "f014481a4c4d41158078878c0f0bfe3d",
            "4f0e278ffe074f3aa6f8ad5a10de6efa",
            "85483b15d40d477c99e5eedb066d40a3",
            "a73ad231c36a4cd1a09bbbfe19818351",
            "221d00b77e9b43ac9a4ad532203fdcd2",
            "19e16b8e354b493ab41570731426caf3",
            "b38a9f916dbf44e1b2fcd204a6fca5ef",
            "0c2faf1bc7594d0abb84a95d7095ac19",
            "cc27f0d0470e458193d210ffd9526d3d",
            "06684fe667d245aaa6dda3699081538c",
            "240a8a4e47f349658bfff90a85e35472",
            "8e1587742ab34ac58c3be5c7de1e3cab",
            "3bb1ebdd11d548189ffd846798d63535",
            "139093284ffc47bb993410f987c135a6",
            "84db6d98ebf24644a5f2334469d93827",
            "df7d3dc81344475da8d6e6d257f9ae4e",
            "f0308236675f486bae19a8d203797778",
            "cb96ecebcd164704a866d2e5b7d80f65"
          ]
        },
        "id": "e937f1e4",
        "outputId": "3c6ea2f6-b432-451d-adfb-3c0a5077e272"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 5 files:   0%|          | 0/5 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ac288c3fcd24dd4a64aa916a57793e6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d1510e3863ef4cf4aa706ccfa5020753"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/1.43k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab18b50d867349dbbabc090d5a6bcb22"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/650 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc4c15e952ce40e08941a2e271b05c27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef32f4bbf7a84c9db92dc2629dcd2cde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5380943f94340fe96ee4f87adf86aaa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 18 files:   0%|          | 0/18 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2dd1f435bb6842748908b330b52a2ef3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "dutch.txt:   0%|          | 0.00/453 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c55245cc4a49476ebc353f5d54734df5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "arabic.txt:   0%|          | 0.00/6.35k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e162d40ff131449d841504417f8aba08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "french.txt:   0%|          | 0.00/813 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b83b2b86b1d94176b1f6c4f567818396"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "danish.txt:   0%|          | 0.00/424 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a07459ce1aa74dd78bd795b690434d8f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "english.txt:   0%|          | 0.00/936 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2d862fea2a846eaa5bd69adf9687bd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "finnish.txt:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a903506a618645568ae6f46258d03e3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "german.txt:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca4d854a437c43cd9879598e2d231564"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac688c715e14400c8707696411fe9b09"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "greek.txt:   0%|          | 0.00/2.17k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d374e796ab0443379350179a886b4f8c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "italian.txt:   0%|          | 0.00/1.65k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b76e4c731f5542e8b0b858720ca8fc73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "russian.txt:   0%|          | 0.00/1.24k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "237dc598580b489caeb5dc7f74bcbd1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "norwegian.txt:   0%|          | 0.00/851 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5053ac51fead4c39989b9c6f9ddf628b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "hungarian.txt:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b44027596789489ba1639c79c54d3043"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "portuguese.txt:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c1bbb4a08b34ac88d47488a07570cc4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "spanish.txt:   0%|          | 0.00/2.18k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "50970dbe554b4379abbcb0029a34b9b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "romanian.txt:   0%|          | 0.00/1.91k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a97b4dc679b34bd183d28e18131277f5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "swedish.txt:   0%|          | 0.00/559 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09f956902b9c4ac7a35ab050e97fb54e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "turkish.txt:   0%|          | 0.00/260 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c2faf1bc7594d0abb84a95d7095ac19"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Initialize embedding models\n",
        "dense_embedding_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
        "bm25_embedding_model = SparseTextEmbedding(\"Qdrant/bm25\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5eb3d6a2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5eb3d6a2",
        "outputId": "fa791b0c-0452-453a-a2c5-cf7680207f8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (1_000_000,)\n",
            "Series: 'abstract' [str]\n",
            "[\n",
            "\t\"  Haldane predicted an analog \u2026\n",
            "\t\"  This paper examines the effe\u2026\n",
            "\t\"  Adopting Schwinger's formali\u2026\n",
            "\t\"  Community or modular structu\u2026\n",
            "\t\"  The instanton--anti-instanto\u2026\n",
            "\t\u2026\n",
            "\t\"  We analyze electromagnetic w\u2026\n",
            "\t\"  We introduce a method for ef\u2026\n",
            "\t\"  We have revealed new feature\u2026\n",
            "\t\"  A complete system of solutio\u2026\n",
            "\t\"  In a previous work we have p\u2026\n",
            "]\n",
            "Total documents loaded: 1000000\n"
          ]
        }
      ],
      "source": [
        "# Load 1 Million Documents (arxiv abstracts)\n",
        "documents = pl.read_parquet('hf://datasets/bluuebunny/arxiv_abstract_embedding_mxbai_large_v1_milvus_binary/**/*.parquet')\n",
        "documents = documents['abstract'].sample(1000000)\n",
        "print(documents)\n",
        "print(f\"Total documents loaded: {len(documents)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef4b50b2",
      "metadata": {
        "id": "ef4b50b2"
      },
      "source": [
        "### Generate Actual Embeddings\n",
        "\n",
        "Now that our data and models are prepared, we're ready to start generating embeddings. **This is the most compute intensive task in the workflow, and may take some time to complete!**\n",
        "\n",
        "**Time required on CPU**\n",
        "- Dense Embeddings: ~20 seconds per 1000 docs\n",
        "- Sparse Embeddings: ~1 second per 1000 docs\n",
        "\n",
        "**GPU Usage**\n",
        "\n",
        "Before generating embeddings, we check for GPU availability and set appropriate batch sizes. GPU acceleration significantly speeds up embedding generation, especially for dense embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "sDmIQphr7wUO",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDmIQphr7wUO",
        "outputId": "98a159ee-109d-43b7-d6a9-73829a7735b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "GPU count: 1\n",
            "Current GPU: NVIDIA A100-SXM4-40GB\n"
          ]
        }
      ],
      "source": [
        "# Check GPU availability\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "print(f\"GPU count: {torch.cuda.device_count()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"Current GPU: {torch.cuda.get_device_name()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ZPPljGG94NyK",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZPPljGG94NyK",
        "outputId": "7682312f-5e64-4346-987e-5369fe963f14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch size: 200\n"
          ]
        }
      ],
      "source": [
        "# Set batch_sized based on GPU availability\n",
        "if torch.cuda.is_available():\n",
        "    batch_size = 200\n",
        "else:\n",
        "    batch_size = 32\n",
        "\n",
        "print(f\"Batch size: {batch_size}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b527d84a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b527d84a",
        "outputId": "93e66161-0b9b-4e94-9c4e-766266eb4314"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Dense Embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000000/1000000 [2:19:15<00:00, 119.68doc/s]\n",
            "Sparse Embeddings: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1000000/1000000 [05:49<00:00, 2862.53doc/s]\n"
          ]
        }
      ],
      "source": [
        "# Generate embeddings for all documents; use batching to optimize performance\n",
        "print(\"Generating embeddings...\")\n",
        "\n",
        "def generate_embeddings(model, documents, batch_size=batch_size, desc=\"Embeddings\"):\n",
        "    embeddings = []\n",
        "\n",
        "    with tqdm(total=len(documents), desc=desc, unit=\"doc\") as pbar:\n",
        "        for i in range(0, len(documents), batch_size):\n",
        "            batch = documents[i:i + batch_size]\n",
        "            batch_embeddings = list(model.embed(batch))\n",
        "            embeddings.extend(batch_embeddings)\n",
        "            pbar.update(len(batch))\n",
        "\n",
        "    return embeddings\n",
        "\n",
        "dense_embeddings = generate_embeddings(\n",
        "    dense_embedding_model, documents, batch_size=batch_size, desc=\"Dense Embeddings\"\n",
        ")\n",
        "\n",
        "bm25_embeddings = generate_embeddings(\n",
        "    bm25_embedding_model, documents, batch_size=batch_size, desc=\"Sparse Embeddings\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "643473ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "643473ed",
        "outputId": "885acbbf-fab3-44ab-eadb-d6dbfbedd155"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dense embedding shape: (384,)\n",
            "BM25 embedding type: <class 'fastembed.sparse.sparse_embedding_base.SparseEmbedding'>\n"
          ]
        }
      ],
      "source": [
        "# Check shapes and types\n",
        "print(f\"Dense embedding shape: {dense_embeddings[0].shape}\")\n",
        "print(f\"BM25 embedding type: {type(bm25_embeddings[0])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8960dd57",
      "metadata": {
        "id": "8960dd57"
      },
      "source": [
        "## Using Qdrant Cloud Vector Database\n",
        "\n",
        "Qdrant is a high-performance vector database optimized for similarity search. We're using it because has:\n",
        "\n",
        "- Multi-vector support\n",
        "- Hybrid search\n",
        "- Filtering\n",
        "- Scalability\n",
        "- Performance\n",
        "- Cloud hosting\n",
        "\n",
        "(and of course, because it's required for the project!)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61eae127",
      "metadata": {
        "id": "61eae127"
      },
      "source": [
        "### Setting up Qdrant\n",
        "\n",
        "We are using Qdrant Cloud for this excercise, which requires an endpoint and API key to access the cluster. The code below prompts the user for the information (rather than hardcode, presenting security risks), and then creates a [collection](https://qdrant.tech/documentation/concepts/collections/). A collection is a named set of points (vectors with a payload) among which you can search. Binary quantization is used to reduce memory usage while maintaining search quality. Lastly, a [tenant index](https://qdrant.tech/documentation/guides/multiple-partitions/#configure-multitenancy) is also created to allow filtering by user_id."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a376d412",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a376d412",
        "outputId": "392f1009-164b-4652-b266-027bf2963601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Qdrant endpoint: https://16325403-8fa2-40e5-8236-03ad0b059833.us-west-2-0.aws.cloud.qdrant.io\n",
            "Qdrant API key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
          ]
        }
      ],
      "source": [
        "# Configure up Qdrant endpoint and API key\n",
        "QDRANT_ENDPOINT = (\n",
        "    os.environ[\"QDRANT_ENDPOINT\"]\n",
        "    if \"QDRANT_ENDPOINT\" in os.environ\n",
        "    else input(\"Qdrant endpoint: \")\n",
        ")\n",
        "QDRANT_API_KEY = (\n",
        "    os.environ[\"QDRANT_API_KEY\"]\n",
        "    if \"QDRANT_API_KEY\" in os.environ\n",
        "    else getpass(\"Qdrant API key: \")\n",
        ")\n",
        "\n",
        "COLLECTION_NAME = \"hybrid-search\"\n",
        "\n",
        "# Make connection\n",
        "client = QdrantClient(\n",
        "    url=QDRANT_ENDPOINT,\n",
        "    api_key=QDRANT_API_KEY\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "fffde00a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fffde00a",
        "outputId": "01f9c7bf-e675-488a-c650-cd170d9987ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deleted existing collection: hybrid-search\n",
            "Created new collection: hybrid-search\n"
          ]
        }
      ],
      "source": [
        "# Delete existing collection if it exists\n",
        "try:\n",
        "    client.delete_collection(COLLECTION_NAME)\n",
        "    print(f\"Deleted existing collection: {COLLECTION_NAME}\")\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Create collection\n",
        "client.create_collection(\n",
        "    COLLECTION_NAME,\n",
        "    vectors_config={\n",
        "        \"dense\": models.VectorParams(\n",
        "            size=384,\n",
        "            distance=models.Distance.COSINE,\n",
        "            quantization_config=models.BinaryQuantization(\n",
        "                binary=models.BinaryQuantizationConfig(\n",
        "                    always_ram=True\n",
        "                )\n",
        "            )\n",
        "        ),\n",
        "    },\n",
        "    sparse_vectors_config={\n",
        "        \"bm25\": models.SparseVectorParams(modifier=models.Modifier.IDF)\n",
        "    }\n",
        ")\n",
        "print(f\"Created new collection: {COLLECTION_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "aca56d50",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aca56d50",
        "outputId": "350dc261-e703-4ea6-e503-5f8cc3d8123e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "UpdateResult(operation_id=1, status=<UpdateStatus.COMPLETED: 'completed'>)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "# Create user_id index for filtering\n",
        "client.create_payload_index(\n",
        "    collection_name=COLLECTION_NAME,\n",
        "    field_name=\"user_id\",\n",
        "    field_schema=models.KeywordIndexParams(\n",
        "        type=models.KeywordIndexType.KEYWORD,\n",
        "        is_tenant=True,\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1e4f0390",
      "metadata": {
        "id": "1e4f0390"
      },
      "source": [
        "### Point Creation\n",
        "Each \"point\" in Qdrant represents a document with all its associated vectors and metadata:\n",
        "\n",
        "#### Point Structure:\n",
        "- **ID**: Unique identifier for the document  \n",
        "- **Vectors**: Both embedding types stored together  \n",
        "- **Payload**: Document text and metadata (including user_id for filtering)  \n",
        "\n",
        "#### Simulated Multi-User Environment:\n",
        "We're randomly assigning user IDs (`user_1` through `user_10`) to simulate a multi-tenant application where users should only see their own documents.\n",
        "\n",
        "**This structure enables both vector similarity search and metadata filtering in a single query.**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "1f8200d3",
      "metadata": {
        "id": "1f8200d3"
      },
      "outputs": [],
      "source": [
        "from qdrant_client.models import PointStruct\n",
        "\n",
        "# Point creation\n",
        "points = []\n",
        "for idx, (dense_embedding, bm25_embedding, doc) in enumerate(\n",
        "    zip(dense_embeddings, bm25_embeddings, documents)\n",
        "):\n",
        "    # Generate a random user_id for demo\n",
        "    user_id = f\"user_{random.randint(1, 10)}\"\n",
        "\n",
        "    point = PointStruct(\n",
        "        id=idx,\n",
        "        vector={\n",
        "            \"dense\": dense_embedding.tolist(),\n",
        "            \"bm25\": bm25_embedding.as_object(),\n",
        "        },\n",
        "        payload={\n",
        "            \"document\": doc,\n",
        "            \"user_id\": user_id\n",
        "        }\n",
        "    )\n",
        "    points.append(point)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "638f3b7e",
      "metadata": {
        "id": "638f3b7e"
      },
      "source": [
        "### Ingesting Data with Qdrant\n",
        "\n",
        "Here, we send the data to our Qdrant vector database cluster using a memory-and-network balanced `batch_size` for performance optimization."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "48b9efbc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48b9efbc",
        "outputId": "5df2c3c8-72e1-45a4-e292-afe198d9a0a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Uploading to Qdrant: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 40000/40000 [2:14:44<00:00,  4.95it/s]\n"
          ]
        }
      ],
      "source": [
        "# Batch upsert for better performance\n",
        "batch_size = 25\n",
        "for i in tqdm(range(0, len(points), batch_size), desc=\"Uploading to Qdrant\"):\n",
        "    batch = points[i:i + batch_size]\n",
        "    client.upsert(collection_name=COLLECTION_NAME, points=batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c275e64f",
      "metadata": {
        "id": "c275e64f"
      },
      "source": [
        "### Retrieve Vectors from Qdrant\n",
        "\n",
        "Now for the fun stuff! Let's see how well we can retrieve content from Qdrant using a query.\n",
        "\n",
        "The query is both specific and general, with semantic meaning. This is something a traditional database would not be able to handle effectively. The results are filtered for hypothetical `user_3`.\n",
        "\n",
        "#### Query Strategy:\n",
        "1. Convert the query into both dense and sparse embeddings\n",
        "2. Use these embeddings to find candidate documents\n",
        "3. Apply user filtering for security\n",
        "4. Rerank results for optimal relevance  \n",
        "\n",
        "This approach combines the strengths of all three embedding methods while maintaining security boundaries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "340b4579",
      "metadata": {
        "id": "340b4579"
      },
      "outputs": [],
      "source": [
        "# Enter query and user_id filter\n",
        "query = \"What are the most interesting galaxies in the universe?\"\n",
        "target_user_id = \"user_3\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ae2f59",
      "metadata": {
        "id": "e8ae2f59"
      },
      "source": [
        "The query itself must be converted to an embedding so that Approximate Nearest Neighbor (ANN) search can find the most similar content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "3ebbd359",
      "metadata": {
        "id": "3ebbd359"
      },
      "outputs": [],
      "source": [
        "# Embed the query into vector space\n",
        "dense_vectors = next(dense_embedding_model.query_embed(query))\n",
        "sparse_vectors = next(bm25_embedding_model.query_embed(query))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2dc05d1e",
      "metadata": {
        "id": "2dc05d1e"
      },
      "outputs": [],
      "source": [
        "# Create prefetch that will find candidate documents from hybrid search\n",
        "prefetch = [\n",
        "        models.Prefetch(\n",
        "            query=dense_vectors,\n",
        "            using=\"dense\",\n",
        "            limit=50,\n",
        "        ),\n",
        "        models.Prefetch(\n",
        "            query=models.SparseVector(**sparse_vectors.as_object()),\n",
        "            using=\"bm25\",\n",
        "            limit=50,\n",
        "        ),\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5f149098",
      "metadata": {
        "id": "5f149098"
      },
      "outputs": [],
      "source": [
        "# Create user_id filter using the indexed field\n",
        "user_filter = models.Filter(\n",
        "    must=[\n",
        "        models.FieldCondition(\n",
        "            key=\"user_id\",\n",
        "            match=models.MatchValue(value=target_user_id)\n",
        "        )\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "6cb1252c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cb1252c",
        "outputId": "d570b44e-cbda-4c5c-9ab8-222ae40083a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieved 98 candidates from hybrid search\n"
          ]
        }
      ],
      "source": [
        "# Run hybrid search to get candidates\n",
        "candidates = client.query_points(\n",
        "    COLLECTION_NAME,\n",
        "    prefetch=prefetch,\n",
        "    query=models.FusionQuery(fusion=models.Fusion.RRF),\n",
        "    with_payload=True,\n",
        "    limit=500,  # Get decent volume of candidates for reranking\n",
        "    query_filter=user_filter\n",
        ")\n",
        "\n",
        "print(f\"Retrieved {len(candidates.points)} candidates from hybrid search\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sssS4nSsOaSo",
      "metadata": {
        "id": "sssS4nSsOaSo"
      },
      "source": [
        "## Reranking\n",
        "The initial hybrid search retrieval casts a wide net quickly, then reranking applies sophisticated scoring to improve the final results. It's faster than running expensive models on our entire corpus, but more accurate than relying only on simple similarity.\n",
        "\n",
        "Here, we first extract candidate documents (with metadata) from our hybrid search results. Then we rerank them using the Rerankers library.\n",
        "\n",
        "**Note:** Not all rerankers create embeddings, but the ColBERT reranker used here does (at a token-level not document-level)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "tKXdp3mvQk0W",
      "metadata": {
        "id": "tKXdp3mvQk0W"
      },
      "outputs": [],
      "source": [
        "# Extract documents and their metadata for reranking\n",
        "candidate_docs = []\n",
        "candidate_metadata = []\n",
        "\n",
        "for point in candidates.points:\n",
        "    candidate_docs.append(point.payload.get('document', ''))\n",
        "    candidate_metadata.append({\n",
        "        'id': point.id,\n",
        "        'score': point.score,\n",
        "        'user_id': point.payload.get('user_id', ''),\n",
        "        'document': point.payload.get('document', '')\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "ZLVKgjNaQoIT",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLVKgjNaQoIT",
        "outputId": "c3e6d4a2-0bef-4b4c-a892-d7c416bc07e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reranking with ColBERT...\n",
            "Loading default colbert model for language en\n",
            "Default Model: colbert-ir/colbertv2.0\n",
            "Loading ColBERTRanker model colbert-ir/colbertv2.0 (this message can be suppressed by setting verbose=0)\n",
            "No device set\n",
            "Using device cuda\n",
            "No dtype set\n",
            "Using dtype torch.float32\n",
            "Loading model colbert-ir/colbertv2.0, this might take a while...\n",
            "Linear Dim set to: 128 for downcasting\n"
          ]
        }
      ],
      "source": [
        "# Rerank the candidate documents from hybrid search\n",
        "print(\"Reranking with ColBERT...\")\n",
        "ranker = Reranker('colbert')\n",
        "reranked_results = ranker.rank(query=query, docs=candidate_docs)\n",
        "\n",
        "# Get top 50 reranked results\n",
        "top_results = reranked_results.top_k(50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb73c33",
      "metadata": {
        "id": "1eb73c33"
      },
      "source": [
        "### Display Results\n",
        "\n",
        "To better analyze our results, I've cleaned them up here into a pretty format from Polars."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "41edf645",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "41edf645",
        "outputId": "c8703736-2ea1-4d83-b175-331b39e4c890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape: (50, 4)\n",
            "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
            "\u2502 id     \u2506 score    \u2506 user_id \u2506 payload                                                            \u2502\n",
            "\u2502 ---    \u2506 ---      \u2506 ---     \u2506 ---                                                                \u2502\n",
            "\u2502 i64    \u2506 f64      \u2506 str     \u2506 str                                                                \u2502\n",
            "\u255e\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u256a\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2550\u2561\n",
            "\u2502 958261 \u2506 0.752109 \u2506 user_3  \u2506 Luminous red galaxies (LRGs) are the most massive galaxies at      \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 $z\\sim 0.5$ and, by selection, have negligible star formation.     \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 These objects have halo masses between those of $L_{*}$ galaxies,  \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 whose c\u2026                                                           \u2502\n",
            "\u2502 674805 \u2506 0.729608 \u2506 user_3  \u2506 When and how did galaxies form and their metals accumulate? Over   \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 the last decade, this has moved from an archeological question to  \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 a live investigation: there is now a broad picture of the          \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 evolution\u2026                                                         \u2502\n",
            "\u2502 261214 \u2506 0.726807 \u2506 user_3  \u2506 Giant radio sources (GRSs) defined to be > 0.7 Mpc are the largest \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 single objects in the Universe and can be associated with both     \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 galaxies (GRGs) and quasars (GRQs). They are important for         \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 understan\u2026                                                         \u2502\n",
            "\u2502 9175   \u2506 0.713623 \u2506 user_3  \u2506 First-generation (Population III) stars in the universe play an    \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 important role inearly enrichment of heavy elements in galaxies    \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 and intergalactic medium and thus affect the history of galaxies.  \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 The \u2026                                                              \u2502\n",
            "\u2502 471297 \u2506 0.70983  \u2506 user_3  \u2506 Damped Ly_alpha galaxies provide a sample of young galaxies where  \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 chemical abundances can be derived throughout the whole universe   \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 with an accuracy comparable to that for the local universe.        \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 Despite\u2026                                                           \u2502\n",
            "\u2502 \u2026      \u2506 \u2026        \u2506 \u2026       \u2506 \u2026                                                                  \u2502\n",
            "\u2502 171163 \u2506 0.608551 \u2506 user_3  \u2506 Although nearly one-third of barred galaxies host an inner,        \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 secondary bar, the formation and evolution of double barred        \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 galaxies remain unclear. We show here an example model of a        \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 galaxy, dominated \u2026                                                \u2502\n",
            "\u2502 988754 \u2506 0.60836  \u2506 user_3  \u2506 We detect the weak gravitational lensing distortion of 450,000     \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 background galaxies (20<R<23) by 790 foreground galaxies (R<18)    \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 selected from the Las Campanas Redshift Survey (LCRS). This is the \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 firs\u2026                                                              \u2502\n",
            "\u2502 272688 \u2506 0.608162 \u2506 user_3  \u2506 The baryonic Tully-Fisher relation (BTFR), which connects the      \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 baryonic mass of galaxies with their circular velocities, has been \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 validated across a wide range of galaxies, from dwarf galaxies to  \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 mas\u2026                                                               \u2502\n",
            "\u2502 494101 \u2506 0.608157 \u2506 user_3  \u2506 Cosmic Dawn Intensity Mapper is a \"Probe Class\" mission concept    \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 for reionization studies of the universe. It will be capable of    \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 spectroscopic imaging observations between 0.7 to 6-7 microns in   \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 the n\u2026                                                             \u2502\n",
            "\u2502 668438 \u2506 0.608085 \u2506 user_3  \u2506 Using the unique dataset obtained within the course of the SAURON  \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 project, a radically new view of the structure, dynamics and       \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 stellar populations of early-type galaxies has emerged. We show    \u2502\n",
            "\u2502        \u2506          \u2506         \u2506 that ga\u2026                                                           \u2502\n",
            "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n"
          ]
        }
      ],
      "source": [
        "# Combine reranking results with original metadata\n",
        "final_results = []\n",
        "for result in top_results:\n",
        "    original_metadata = candidate_metadata[result.doc_id]\n",
        "    final_results.append({\n",
        "        'id': original_metadata['id'],\n",
        "        'score': result.score,  # ColBERT reranking score\n",
        "        'user_id': original_metadata['user_id'],\n",
        "        'payload': original_metadata['document']\n",
        "    })\n",
        "\n",
        "df = pl.DataFrame(final_results)\n",
        "pl.Config.set_fmt_str_lengths(200) # Show up to 200 characters from abstract\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49fbd4ac",
      "metadata": {
        "id": "49fbd4ac"
      },
      "source": [
        "## Conclusion and Takeaways\n",
        "\n",
        "### What We've Accomplished\n",
        "\n",
        "**Multi-Modal Embeddings**  \n",
        "- Dense embeddings for semantic understanding  \n",
        "- Sparse embeddings for keyword precision  \n",
        "- Late interaction embeddings for fine-grained relevance  \n",
        "\n",
        "**Production-Ready Architecture**  \n",
        "- Scalable vector database with Qdrant  \n",
        "- Efficient batch processing and indexing  \n",
        "- Multi-tenant security with user filtering  \n",
        "\n",
        "**Advanced Search Capabilities**  \n",
        "- Hybrid retrieval combining multiple signals  \n",
        "- Sophisticated ranking and reranking  \n",
        "- Flexible query processing pipeline  \n",
        "\n",
        "### Potential Enhancements\n",
        "\n",
        "- Implement query-time filtering for better performance  \n",
        "- Add caching for frequently accessed embeddings\n",
        "- Parallelize long-running processes for faster execution\n",
        "- Integrate with LLMs for a more conversation experience (RAG)\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "1. **Hybrid approaches outperform single methods** by combining different strengths  \n",
        "2. **Late interaction models** provide exceptional precision for text search  \n",
        "3. **Vector databases** enable sophisticated multi-modal search at scale  \n",
        "4. **Security considerations** are crucial for multi-tenant applications  \n",
        "\n",
        "We've built a hybrid search system capable of providing a solid foundation for building production-grade search applications that deliver both high recall and precision across diverse query types.\n",
        "\n",
        "Thanks for the fun project!\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}