{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5f1a5c2",
   "metadata": {},
   "source": [
    "# Hybrid Search with Reranking and Filtering\n",
    "_Last updated: May 31, 2025_\n",
    "\n",
    "This notebook demonstrates advanced hybrid search techniques combining three different [embedding](https://en.wikipedia.org/wiki/Word_embedding) approaches: **dense**, **sparse**, and **late interaction**. It uses dense and sparse embeddings for initial retrieval, then late interaction for reranking. It also filters by `user_id` to simulate multitenancy.\n",
    "\n",
    "### What is Hybrid Search?\n",
    "Hybrid search combines multiple retrieval methods to improve search quality and relevance. Instead of relying on a single approach, we leverage the strengths of different embedding techniques:\n",
    "\n",
    "- **Dense Embeddings** (Semantic Search): Uses neural networks to create dense vector representations of unstructured data, capturing semantic meaning and context. Great for finding conceptually similar content.\n",
    "\n",
    "- **Sparse Embeddings** (Keyword Search): Uses keyword-based search through BM25 for finding exact matches in words. It's great for maintaining interpretability and precision, especially in use cases where industry-specific terms are used.\n",
    "\n",
    "- **Late Interaction Embeddings** (Advanced Semantic): Allows each [token](https://en.wikipedia.org/wiki/Text_segmentation#Word_segmentation) to get it's own embedding, enabling precise matching and fine-grained interactions. It combines the benefits of dense retrieval and token-level precision, making it great for reranking in this workflow.\n",
    "\n",
    "**Why?** By combining these three approaches, we get more robustness to different types of queries to the vector database\n",
    "\n",
    "### References and Resources\n",
    "The following were used to complete this project:\n",
    "- [Qdrant Reranking in Hybrid Search](https://qdrant.tech/documentation/advanced-tutorials/reranking-hybrid-search/?q=ingest#ingestion-stage)\n",
    "- [Qdrant 'Concepts' Documentation](https://qdrant.tech/documentation/concepts/)\n",
    "- [How to Build the Ultimate Hybrid Search with Qdrant (video)](https://www.youtube.com/live/LAZOxqzceEU?si=4HF34v9G1xq3Z3-6)\n",
    "- [Anthropic's Claude](https://claude.ai/new) (for coding support like troubleshooting, debugging, cleanup)\n",
    "- [Hugging Face Datasets](https://huggingface.co/datasets?modality=modality:text&sort=trending) (for source documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a8135",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "First we install all the required libraries for this notebook:\n",
    "\n",
    "- **qdrant-client**: Qdrant's vector database client for storing and retrieving embeddings\n",
    "- **fastembed**: Qdrant's opensource, lightweight, and comprehensive library for generating different embedding types\n",
    "- **tqdm**: Visual progress bars to track long-running operations\n",
    "- **polars**: Fast dataframe library for data manipulation\n",
    "\n",
    "We then import all the libraries and modules we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3922b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: qdrant-client in c:\\program files\\python313\\lib\\site-packages (1.14.2)\n",
      "Requirement already satisfied: fastembed in c:\\program files\\python313\\lib\\site-packages (0.7.0)\n",
      "Requirement already satisfied: tqdm in c:\\program files\\python313\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: polars in c:\\program files\\python313\\lib\\site-packages (1.30.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in c:\\program files\\python313\\lib\\site-packages (from qdrant-client) (1.71.0)\n",
      "Requirement already satisfied: httpx>=0.20.0 in c:\\program files\\python313\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (0.28.1)\n",
      "Requirement already satisfied: numpy>=2.1.0 in c:\\program files\\python313\\lib\\site-packages (from qdrant-client) (2.2.6)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in c:\\program files\\python313\\lib\\site-packages (from qdrant-client) (2.10.1)\n",
      "Requirement already satisfied: protobuf>=3.20.0 in c:\\program files\\python313\\lib\\site-packages (from qdrant-client) (6.31.1)\n",
      "Requirement already satisfied: pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8 in c:\\program files\\python313\\lib\\site-packages (from qdrant-client) (2.11.5)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in c:\\program files\\python313\\lib\\site-packages (from qdrant-client) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.20 in c:\\program files\\python313\\lib\\site-packages (from fastembed) (0.32.3)\n",
      "Requirement already satisfied: loguru<0.8.0,>=0.7.2 in c:\\program files\\python313\\lib\\site-packages (from fastembed) (0.7.3)\n",
      "Requirement already satisfied: mmh3<6.0.0,>=4.1.0 in c:\\program files\\python313\\lib\\site-packages (from fastembed) (5.1.0)\n",
      "Requirement already satisfied: onnxruntime>1.20.0 in c:\\program files\\python313\\lib\\site-packages (from fastembed) (1.22.0)\n",
      "Requirement already satisfied: pillow<12.0.0,>=10.3.0 in c:\\program files\\python313\\lib\\site-packages (from fastembed) (11.2.1)\n",
      "Requirement already satisfied: py-rust-stemmers<0.2.0,>=0.1.0 in c:\\program files\\python313\\lib\\site-packages (from fastembed) (0.1.5)\n",
      "Requirement already satisfied: requests<3.0,>=2.31 in c:\\program files\\python313\\lib\\site-packages (from fastembed) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<1.0,>=0.15 in c:\\program files\\python313\\lib\\site-packages (from fastembed) (0.21.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\e_nkanungo\\appdata\\roaming\\python\\python313\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: anyio in c:\\program files\\python313\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (4.9.0)\n",
      "Requirement already satisfied: certifi in c:\\program files\\python313\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\program files\\python313\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\program files\\python313\\lib\\site-packages (from httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\program files\\python313\\lib\\site-packages (from httpcore==1.*->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (0.16.0)\n",
      "Requirement already satisfied: h2<5,>=3 in c:\\program files\\python313\\lib\\site-packages (from httpx[http2]>=0.20.0->qdrant-client) (4.2.0)\n",
      "Requirement already satisfied: filelock in c:\\program files\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (3.18.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\program files\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (2025.3.0)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\e_nkanungo\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\program files\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\program files\\python313\\lib\\site-packages (from huggingface-hub<1.0,>=0.20->fastembed) (4.13.2)\n",
      "Requirement already satisfied: win32-setctime>=1.0.0 in c:\\program files\\python313\\lib\\site-packages (from loguru<0.8.0,>=0.7.2->fastembed) (1.2.0)\n",
      "Requirement already satisfied: coloredlogs in c:\\program files\\python313\\lib\\site-packages (from onnxruntime>1.20.0->fastembed) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in c:\\program files\\python313\\lib\\site-packages (from onnxruntime>1.20.0->fastembed) (25.2.10)\n",
      "Requirement already satisfied: sympy in c:\\program files\\python313\\lib\\site-packages (from onnxruntime>1.20.0->fastembed) (1.14.0)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\e_nkanungo\\appdata\\roaming\\python\\python313\\site-packages (from portalocker<3.0.0,>=2.7.0->qdrant-client) (310)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\program files\\python313\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\program files\\python313\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\program files\\python313\\lib\\site-packages (from pydantic!=2.0.*,!=2.1.*,!=2.2.0,>=1.10.8->qdrant-client) (0.4.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\program files\\python313\\lib\\site-packages (from requests<3.0,>=2.31->fastembed) (3.4.2)\n",
      "Requirement already satisfied: hyperframe<7,>=6.1 in c:\\program files\\python313\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (6.1.0)\n",
      "Requirement already satisfied: hpack<5,>=4.1 in c:\\program files\\python313\\lib\\site-packages (from h2<5,>=3->httpx[http2]>=0.20.0->qdrant-client) (4.1.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\program files\\python313\\lib\\site-packages (from anyio->httpx>=0.20.0->httpx[http2]>=0.20.0->qdrant-client) (1.3.1)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in c:\\program files\\python313\\lib\\site-packages (from coloredlogs->onnxruntime>1.20.0->fastembed) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\program files\\python313\\lib\\site-packages (from sympy->onnxruntime>1.20.0->fastembed) (1.3.0)\n",
      "Requirement already satisfied: pyreadline3 in c:\\program files\\python313\\lib\\site-packages (from humanfriendly>=9.1->coloredlogs->onnxruntime>1.20.0->fastembed) (3.5.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.0.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install required libraries\n",
    "%pip install qdrant-client fastembed tqdm polars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "49c2cd1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Program Files\\Python313\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import dependencies\n",
    "from qdrant_client import QdrantClient, models\n",
    "from fastembed import TextEmbedding, LateInteractionTextEmbedding, SparseTextEmbedding \n",
    "import polars as pl\n",
    "from tqdm import tqdm\n",
    "from getpass import getpass\n",
    "import random, os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a3e6f4",
   "metadata": {},
   "source": [
    "## Create Embeddings\n",
    "\n",
    "This section covers the creation of our three different types of embeddings. Each embedding type captures different aspects of the source text:\n",
    "\n",
    "- **Dense embeddings**: Fixed-size vectors, mostly non-zero, capturing semantic meaning\n",
    "- **Sparse embeddings**: Variable-size vectors, mostly zero, capturing keyword term-frequency information.\n",
    "- **Late interaction embeddings**: Multiple vectors per document for precise relevance with contextual understanding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f9ef5e",
   "metadata": {},
   "source": [
    "### Embedding Setup\n",
    "\n",
    "We must first initialize our embedding models, then we can load our document dataset to start embedding. The source documents for this project were **arxiv paper abstracts**, found on [Hugging Face](https://huggingface.co/datasets/bluuebunny/arxiv_abstract_embedding_mxbai_large_v1_milvus_binary) and published by Mitanshu Sukhwani in 2025. This dataset provides a rich corpus of scientific text for demonstrating text retrieval. We random sample 1 million abstracts from this dataset to reduce overall embeddings to the minimum required volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e937f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding models\n",
    "dense_embedding_model = TextEmbedding(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "bm25_embedding_model = SparseTextEmbedding(\"Qdrant/bm25\")\n",
    "late_interaction_embedding_model = LateInteractionTextEmbedding(\"colbert-ir/colbertv2.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb3d6a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (1_000,)\n",
      "Series: 'abstract' [str]\n",
      "[\n",
      "\t\"  We present a combined experi…\n",
      "\t\"  In a companion paper we have…\n",
      "\t\"  A recent work has shown that…\n",
      "\t\"  Thermoelectric (TE) conversi…\n",
      "\t\"  The emergence of flat bands …\n",
      "\t…\n",
      "\t\"  A nonequilibrium Green's fun…\n",
      "\t\"  In this paper, we propose a …\n",
      "\t\"  The ALICE experiment at LHC …\n",
      "\t\"  We generalize the scattering…\n",
      "\t\"  Aims. We tested the new atom…\n",
      "]\n",
      "Total documents loaded: 1000\n"
     ]
    }
   ],
   "source": [
    "# Load 1 Million Documents (abstracts)\n",
    "documents = pl.read_parquet('hf://datasets/bluuebunny/arxiv_abstract_embedding_mxbai_large_v1_milvus_binary/**/*.parquet')\n",
    "documents = documents['abstract'].sample(1000)\n",
    "print(documents)\n",
    "print(f\"Total documents loaded: {len(documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4b50b2",
   "metadata": {},
   "source": [
    "### Generate Actual Embeddings\n",
    "\n",
    "Now that our data and models are prepared, we're ready to start generating embeddings. **This is the most compute intensive task in the workflow, and may take some time to complete!**\n",
    "\n",
    "**Time required on CPU**\n",
    "- Dense Embeddings: ~20 seconds per 1000 docs\n",
    "- Sparse Embeddings: ~1 second per 1000 docs\n",
    "- Late Interaction Embeddings: 3~4 minutes per 1000 docs (token-level embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b527d84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating embeddings...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dense Embeddings: 100%|██████████| 1000/1000 [00:19<00:00, 50.64it/s]\n",
      "BM25 Embeddings: 100%|██████████| 1000/1000 [00:00<00:00, 3425.00it/s]\n",
      "Late Interaction Embeddings: 100%|██████████| 1000/1000 [03:24<00:00,  4.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# Generate embeddings for all documents\n",
    "print(\"Generating embeddings...\")\n",
    "\n",
    "dense_embeddings = []\n",
    "for doc in tqdm(documents, desc=\"Dense Embeddings\"):\n",
    "    embedding = next(dense_embedding_model.embed(doc))\n",
    "    dense_embeddings.append(embedding)\n",
    "\n",
    "bm25_embeddings = []\n",
    "for doc in tqdm(documents, desc=\"BM25 Embeddings\"):\n",
    "    embedding = next(bm25_embedding_model.embed(doc))\n",
    "    bm25_embeddings.append(embedding)\n",
    "\n",
    "late_interaction_embeddings = []\n",
    "for doc in tqdm(documents, desc=\"Late Interaction Embeddings\"):\n",
    "    embedding = next(late_interaction_embedding_model.embed(doc))\n",
    "    late_interaction_embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "643473ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense embedding shape: (384,)\n",
      "BM25 embedding type: <class 'fastembed.sparse.sparse_embedding_base.SparseEmbedding'>\n",
      "Late interaction embedding shape: (318, 128)\n"
     ]
    }
   ],
   "source": [
    "# Check shapes and types\n",
    "print(f\"Dense embedding shape: {dense_embeddings[0].shape}\")\n",
    "print(f\"BM25 embedding type: {type(bm25_embeddings[0])}\")\n",
    "print(f\"Late interaction embedding shape: {late_interaction_embeddings[0].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8960dd57",
   "metadata": {},
   "source": [
    "## Using Qdrant Cloud Vector Database\n",
    "\n",
    "Qdrant is a high-performance vector database optimized for similarity search. We're using it because has:\n",
    "\n",
    "- Multi-vector support\n",
    "- Hybrid search\n",
    "- Filtering\n",
    "- Scalability\n",
    "- Performance\n",
    "\n",
    "(and of course, because it's required for the project!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61eae127",
   "metadata": {},
   "source": [
    "### Setting up Qdrant\n",
    "\n",
    "We are using Qdrant Cloud for this excercise, which requires an endpoint and API key to access the cluster. The code below prompts the user for the information (rather than hardcode, presenting security risks), and then creates a [collection](https://qdrant.tech/documentation/concepts/collections/). A collection is a named set of points (vectors with a payload) among which you can search. A [tenant index](https://qdrant.tech/documentation/guides/multiple-partitions/#configure-multitenancy) is also created to allow filtering by user_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a376d412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure up Qdrant endpoint and API key\n",
    "QDRANT_ENDPOINT = (\n",
    "    os.environ[\"QDRANT_ENDPOINT\"]\n",
    "    if \"QDRANT_ENDPOINT\" in os.environ\n",
    "    else input(\"Qdrant endpoint: \")\n",
    ")\n",
    "QDRANT_API_KEY = (\n",
    "    os.environ[\"QDRANT_API_KEY\"]\n",
    "    if \"QDRANT_API_KEY\" in os.environ\n",
    "    else getpass(\"Qdrant API key: \")\n",
    ")\n",
    "\n",
    "COLLECTION_NAME = \"hybrid-search\"\n",
    "\n",
    "# Make connection\n",
    "client = QdrantClient(\n",
    "    url=QDRANT_ENDPOINT,\n",
    "    api_key=QDRANT_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fffde00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection: hybrid-search\n",
      "Created new collection: hybrid-search\n"
     ]
    }
   ],
   "source": [
    "# Delete existing collection if it exists\n",
    "try:\n",
    "    client.delete_collection(COLLECTION_NAME)\n",
    "    print(f\"Deleted existing collection: {COLLECTION_NAME}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Create collection\n",
    "client.create_collection(\n",
    "    COLLECTION_NAME,\n",
    "    vectors_config={\n",
    "        \"dense\": models.VectorParams(\n",
    "            size=384,  # all-MiniLM-L6-v2 embedding size\n",
    "            distance=models.Distance.COSINE,\n",
    "            quantization_config=models.BinaryQuantization(\n",
    "                binary=models.BinaryQuantizationConfig(\n",
    "                    always_ram=True\n",
    "                )\n",
    "            )\n",
    "        ),\n",
    "        \"colbert\": models.VectorParams(\n",
    "            size=128,  # ColBERT embedding dimension\n",
    "            distance=models.Distance.COSINE,\n",
    "            multivector_config=models.MultiVectorConfig(\n",
    "                comparator=models.MultiVectorComparator.MAX_SIM,\n",
    "            )\n",
    "        ),\n",
    "    },\n",
    "    sparse_vectors_config={\n",
    "        \"bm25\": models.SparseVectorParams(modifier=models.Modifier.IDF)\n",
    "    }\n",
    ")\n",
    "print(f\"Created new collection: {COLLECTION_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca56d50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UpdateResult(operation_id=1, status=<UpdateStatus.COMPLETED: 'completed'>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create user_id index for filtering\n",
    "client.create_payload_index(\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    field_name=\"user_id\",\n",
    "    field_schema=models.KeywordIndexParams(\n",
    "        type=models.KeywordIndexType.KEYWORD,\n",
    "        is_tenant=True,\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4f0390",
   "metadata": {},
   "source": [
    "### Point Creation\n",
    "Each \"point\" in Qdrant represents a document with all its associated vectors and metadata:\n",
    "\n",
    "#### Point Structure:\n",
    "- **ID**: Unique identifier for the document  \n",
    "- **Vectors**: All three embedding types stored together  \n",
    "- **Payload**: Document text and metadata (including user_id for filtering)  \n",
    "\n",
    "#### Simulated Multi-User Environment:\n",
    "We're randomly assigning user IDs (`user_1` through `user_10`) to simulate a multi-tenant application where users should only see their own documents.\n",
    "\n",
    "#### Vector Format Conversion:\n",
    "- **Dense**: Convert numpy array to list  \n",
    "- **BM25**: Use `.as_object()` method for sparse format  \n",
    "- **Late Interaction**: Convert 2D numpy array to nested list  \n",
    "\n",
    "This structure enables both vector similarity search and metadata filtering in a single query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f8200d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.models import PointStruct\n",
    "\n",
    "# Point creation\n",
    "points = []\n",
    "for idx, (dense_embedding, bm25_embedding, late_interaction_embedding, doc) in enumerate(\n",
    "    zip(dense_embeddings, bm25_embeddings, late_interaction_embeddings, documents)\n",
    "):\n",
    "    # Generate a random user_id for demo\n",
    "    user_id = f\"user_{random.randint(1, 10)}\"\n",
    "    \n",
    "    point = PointStruct(\n",
    "        id=idx,\n",
    "        vector={\n",
    "            \"dense\": dense_embedding.tolist(),\n",
    "            \"bm25\": bm25_embedding.as_object(),\n",
    "            \"colbert\": late_interaction_embedding.tolist(),\n",
    "        },\n",
    "        payload={\n",
    "            \"document\": doc,\n",
    "            \"user_id\": user_id\n",
    "        }\n",
    "    )\n",
    "    points.append(point)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "638f3b7e",
   "metadata": {},
   "source": [
    "### Ingesting Data with Qdrant\n",
    "\n",
    "Here, we send the data to our Qdrant vector database cluster using a device-balanced `batch_size` for performance optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48b9efbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Uploading to Qdrant: 100%|██████████| 40/40 [02:14<00:00,  3.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# Batch upsert for better performance\n",
    "batch_size = 25\n",
    "for i in tqdm(range(0, len(points), batch_size), desc=\"Uploading to Qdrant\"):\n",
    "    batch = points[i:i + batch_size]\n",
    "    client.upsert(collection_name=COLLECTION_NAME, points=batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c275e64f",
   "metadata": {},
   "source": [
    "### Retrieve Vectors from Qdrant\n",
    "\n",
    "Now for the fun stuff! Let's see how well we can retrieve content from Qdrant using a query.\n",
    "\n",
    "The query is both specific and general, with semantic meaning. This is something a traditional database would not be able to handle effectively. The results are filtered for hypothetical `user_3`.\n",
    "\n",
    "#### Query Strategy:\n",
    "1. **Prefetch** with dense and sparse embeddings to get candidate documents  \n",
    "2. **Rerank** using late interaction embeddings for final precision  \n",
    "3. **Filter** results by user ownership for multi-tenant security  \n",
    "\n",
    "This approach combines the strengths of all three methods while maintaining security boundaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "340b4579",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter query and user_id filter\n",
    "query = \"What are the most interesting galaxies in the universe?\"\n",
    "target_user_id = \"user_3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ae2f59",
   "metadata": {},
   "source": [
    "The query itself must be converted to an embedding so that Approximate Nearest Neighbor (ANN) search can find the most similar content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ebbd359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed the query into vector space\n",
    "dense_vectors = next(dense_embedding_model.query_embed(query))\n",
    "sparse_vectors = next(bm25_embedding_model.query_embed(query))\n",
    "late_vectors = next(late_interaction_embedding_model.query_embed(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dc05d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prefetch that will find candidate documents from multiple vector types\n",
    "prefetch = [\n",
    "        models.Prefetch(\n",
    "            query=dense_vectors,\n",
    "            using=\"dense\",\n",
    "            limit=50,\n",
    "        ),\n",
    "        models.Prefetch(\n",
    "            query=models.SparseVector(**sparse_vectors.as_object()),\n",
    "            using=\"bm25\",\n",
    "            limit=50,\n",
    "        ),\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f149098",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user_id filter using the indexed field\n",
    "user_filter = models.Filter(\n",
    "    must=[\n",
    "        models.FieldCondition(\n",
    "            key=\"user_id\",\n",
    "            match=models.MatchValue(value=target_user_id)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6cb1252c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QueryResponse(points=[ScoredPoint(id=702, version=30, score=16.326674, payload={'document': '  High redshift galaxy clusters have traditionally been a fruitful place to study galaxy evolution. I review various search strategies for finding clusters at z > 1. Most efforts to date have concentrated on the environments of distant AGN. I illustrate these with data on the cluster around 3C 324 (z=1.2) and other, more distant systems, and discuss possibilities for future surveys with large telescopes. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=726, version=31, score=14.451191, payload={'document': '  Using a subsample of 79 nearby clusters from the RASS-SDSS galaxy cluster catalogue of Popesso et al. (2005a), we perform a regression analysis between the cluster integrated star formation rate (Sigma_SFR) the cluster total stellar mass (M_star), the fractions of star forming (f_SF) and blue (f_b) galaxies and other cluster global properties, namely its richness (N_gal, i.e. the total number of cluster members within the cluster virial radius), velocity dispersion (sigma_v), virial mass (M_200), and X-ray luminosity (L_X). All cluster global quantities are corrected for projection effects before the analysis. Galaxy SFRs and stellar masses are taken from the catalog of Brinchmann et al. (2004), which is based on SDSS spectra. We only consider galaxies with M_r <= -20.25 in our analysis, and exclude AGNs. We find that both Sigma_SFR and M_star are correlated with all the cluster global quantities. A partial correlation analysis show that all the correlations are induced by the fundamental one between Sigma_SFR and N_gal, hence there is no evidence that the cluster properties affect the mean SFR or M_star per galaxy. The relations between Sigma_SFR and M_star, on one side, and both N_gal and M_200, on the other side, are linear, i.e. we see no evidence that different clusters have different SFR or different M_star per galaxy and per unit mass. The fraction f_SF does not depend on any cluster property considered, while f_b does depend on L_X. We note that a significant fraction of star-forming cluster galaxies are red (~25% of the whole cluster galaxy population). We conclude that the global cluster properties are unable to affect the SF properties of cluster galaxies, but the presence of the X-ray luminous intra-cluster medium can affect their colors, perhaps through the ram-pressure stripping mechanism. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=632, version=27, score=13.75998, payload={'document': '  We present the detection of compact radio structures of fourteen radio-loud narrow line Seyfert 1 (NLS1) galaxies from Very Long Baseline Array observations at 5 GHz, which were performed in 2013. While 50\\\\% of the sources of our sample show a compact core only, the remaining 50\\\\% exhibit a core-jet structure. The measured brightness temperatures of the cores range from $10^{8.4}$ to $10^{11.4}$ K with a median value of $10^{10.1}$ K, indicating that the radio emission is from non-thermal jets, and that, likely, most sources are not strongly beamed, then implying a lower jet speed in these radio-loud NLS1 galaxies. In combination with archival data taken at multiple frequencies, we find that seven sources show flat or even inverted radio spectra, while steep spectra are revealed in the remaining seven objects. Although all these sources are very radio-loud with $R > 100$, their jet properties are diverse, in terms of their milli-arcsecond (mas) scale (pc scale) morphology and their overall radio spectral shape. The evidence for slow jet speeds (i.e., less relativistic jets), in combination with the low kinetic/radio power, may offer an explanation for the compact VLBA radio structure in most sources. The mildly relativistic jets in these high accretion rate systems are consistent with a scenario, where jets are accelerated from the hot corona above the disk by the magnetic field and the radiation force of the accretion disk. Alternatively, a low jet bulk velocity can be explained by low spin in the Blandford-Znajek mechanism. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=290, version=13, score=12.638304, payload={'document': '  We identify 228 periodic variables in the region of young open cluster NGC 281 using time series photometry carried out from 1 m class ARIES telescopes, Nainital. The cluster membership of these identified variables is determined on the basis colour-magnitude, two colour diagrams and kinematic data. Eighty one variable stars are found to be members of the cluster NGC 281. Of 81 variables, 30 and 51 are probable main-sequence and pre-main-sequence members, respectively. Present study classifies main-sequence variable stars into different variability types according to their periods of variability, shape of light curves and location in the Hertzsprung-Russell diagram. These identified main-sequence variables could be $\\\\beta$ Cep, $\\\\delta$ Scuti, slowly pulsating B type and new class variables. Among 51 pre-main-sequence variables, majority of them are weak line T Tauri stars. The remaining 147 variables could belong to the field population. The variability characteristics of the field population indicate that these variables could be RR Lyrae, $\\\\delta$ Scuti and binaries type variables. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=114, version=6, score=11.706559, payload={'document': \"  The IceCube Neutrino Observatory is the world's largest high energy neutrino telescope, using the Antarctic ice cap as a Cherenkov detector medium. DeepCore, the low energy extension to IceCube, is an infill array with a fiducial volume of around 30 MTon in the deepest, clearest ice, aiming for an energy threshold as low as 10 GeV and extending IceCube's sensitivity to indirect dark matter searches and atmospheric neutrino oscillation physics. We will discuss the analysis of the first year of DeepCore data, as well as ideas for a further extension of the particle physics program in the ice with a future PINGU detector. \", 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=677, version=29, score=11.521705, payload={'document': \"  Observations of Type Ia supernovae used to map the expansion history of the Universe suffer from systematic uncertainties that need to be propagated into the estimates of cosmological parameters. We propose an iterative Monte-Carlo simulation and cosmology fitting technique (SMOCK) to investigate the impact of sources of error upon fits of the dark energy equation of state. This approach is especially useful to track the impact of non-Gaussian, correlated effects, e.g. reddening correction errors, brightness evolution of the supernovae, K-corrections, gravitational lensing, etc. While the tool is primarily aimed for studies and optimization of future instruments, we use the ``Gold'' data-set in Riess et al. (2007) to show examples of potential systematic uncertainties that could exceed the quoted statistical uncertainties. \", 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=773, version=32, score=9.941105, payload={'document': '  Yamamoto et al. (1987) reported the first detection of $c$-C$_3$H radical in {TMC-1} through its transition $2_{1 2} \\\\to 1_{1 1}$ at 91.5 GHz. Mangum and Wootten (1990) detected $c$-C$_3$H through the transition $1_{1 0} \\\\to 1_{1 1}$ at 14.8 GHz in 12 additional galactic objects. The column density of $c$-C$_3$H in the objects was estimated to be about one order of magnitude lower than that of the C$_3$H$_2$ which is ubiquitous in the galactic objects. The most probable production mechanism of both the C$_3$H and C$_3$H$_2$ in dark clouds is a common dissociation reaction of C$_3$H$_3^+$ ion (Adams & Smith, 1987). Although the $c$-C$_3$H is 0.8 eV less stable than its isomer $l$-C$_3$H, finding of comparable column densities of both the isomers in {TMC-1} supports the idea of comparable formation of both the $c$-C$_3$H and $l$-C$_3$H in the cosmic objects. Existence of a metaisomer in interstellar condition is a well known phenomenon in astronomy.   We propose that $c$-C$_3$H may be identified through the transition $3_{3 1} \\\\to 3_{3 0}$ at 3.4 GHz in absorption against the cosmic 2.7 K background in dense cosmic objects when no strong source is present in the background. When there is some strong source in the background of the object, peak of the absorption line decreases with the increase of the strength of the background source. However, at low densities, the intensity is found to increase. Hence, in low density regions, a background source can help in detection of the line. This absorption line may play an important role for identification of $c$-C$_3$H in a large number of cosmic objects. Similar absorption features are found for $c$-C$_3$D radical also. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=497, version=21, score=9.7611885, payload={'document': \"  The charged exotic mesons Z_b(10610) and Z'_b(10650) observed by the Belle collaboration in 2011 are very close to the B*Bbar and B*B*bar thresholds, respectively. This suggests their interpretation as shallow hadronic molecules of B and B* mesons. Using the masses of the Z_b(10610) and Z'_b(10650) as input, we predict the phase shifts for the scattering of B and B* mesons off the exotic mesons Z_b(10610) and Z'_b(10650) to leading order in a non-relativistic effective field theory with contact interactions. Moreover, we rule out the possibility for universal bound states of three B and B* mesons arising from the Efimov effect. \", 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=365, version=16, score=9.623098, payload={'document': '  Clustering is one of the most fundamental and important tasks in data mining. Traditional clustering algorithms, such as K-means, assign every data point to exactly one cluster. However, in real-world datasets, the clusters may overlap with each other. Furthermore, often, there are outliers that should not belong to any cluster. We recently proposed the NEO-K-Means (Non-Exhaustive, Overlapping K-Means) objective as a way to address both issues in an integrated fashion. Optimizing this discrete objective is NP-hard, and even though there is a convex relaxation of the objective, straightforward convex optimization approaches are too expensive for large datasets. A practical alternative is to use a low-rank factorization of the solution matrix in the convex formulation. The resulting optimization problem is non-convex, and we can locally optimize the objective function using an augmented Lagrangian method. In this paper, we consider two fast multiplier methods to accelerate the convergence of an augmented Lagrangian scheme: a proximal method of multipliers and an alternating direction method of multipliers (ADMM). For the proximal augmented Lagrangian or proximal method of multipliers, we show a convergence result for the non-convex case with bound-constrained subproblems. These methods are up to 13 times faster---with no change in quality---compared with a standard augmented Lagrangian method on problems with over 10,000 variables and bring runtimes down from over an hour to around 5 minutes. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=176, version=9, score=9.478168, payload={'document': '  The recent progress in the investigations on the quark structure of the $\\\\Lambda$ and $\\\\Sigma$ baryons are reviewed. It is shown that the quark structure of $\\\\Lambda$ and $\\\\Sigma$ hyperons can provide a new domain to test various theories concerning the spin and flavor structure of the nucleon. The $\\\\Lambda$ and $\\\\Sigma$ Physics might be new directions to explore the quark distributions of baryons both theoretically and experimentally. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=682, version=29, score=9.460367, payload={'document': '  A variety of new and interesting correlated states have been predicted in graphene monolayer doped to Van Hove singularities (VHSs) of its density-of-state (DOS). However, tuning the Fermi energy to reach a VHS of graphene by either gating or chemical doping is prohibitively difficult, owning to their large energy distance (3 eV). Therefore, these correlated states, which arise from effects of strong electron-electron interactions at the VHSs, have remained experimentally elusive. Here, we report experimental evidences of electron-electron interactions around the VHSs of a twisted bilayer graphene (TBG) through scanning tunneling microscopy measurements. By introducing a small twisted angle between two adjacent graphene sheets, we are able to generate low-energy VHSs arbitrarily approaching the Fermi energy. The split of the VHSs are observed and the symmetry breaking of electronic states around the VHSs are directly visualized. These results experimentally demonstrate the important effects of electron-electron interactions on electronic properties around the VHSs of the TBG, therefore providing motivation for further theoretical and experimental studies in graphene systems with considering many-body interactions. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=1, version=2, score=9.33996, payload={'document': '  In a companion paper we have presented many products derived from the application of the spectral synthesis code STARLIGHT to datacubes from the CALIFA survey, including 2D maps of stellar population properties and 1D averages in the temporal and spatial dimensions. Here we evaluate the uncertainties in these products. Uncertainties due to noise and spectral shape calibration errors and to the synthesis method are investigated by means of a suite of simulations based on 1638 CALIFA spectra for NGC 2916, with perturbations amplitudes gauged in terms of the expected errors. A separate study was conducted to assess uncertainties related to the choice of evolutionary synthesis models. We compare results obtained with the Bruzual & Charlot models, a preliminary update of them, and a combination of spectra derived from the Granada and MILES models. About 100k CALIFA spectra are used in this comparison.   Noise and shape-related errors at the level expected for CALIFA propagate to 0.10-0.15 dex uncertainties in stellar masses, mean ages and metallicities. Uncertainties in A_V increase from 0.06 mag in the case of random noise to 0.16 mag for shape errors. Higher order products such as SFHs are more uncertain, but still relatively stable. Due to the large number statistics of datacubes, spatial averaging reduces uncertainties while preserving information on the history and structure of stellar populations. Radial profiles of global properties, as well as SFHs averaged over different regions are much more stable than for individual spaxels. Uncertainties related to the choice of base models are larger than those associated with data and method. Differences in mean age, mass and metallicity are ~ 0.15 to 0.25 dex, and 0.1 mag in A_V. Spectral residuals are ~ 1% on average, but with systematic features of up to 4%. The origin of these features is discussed. (Abridged) ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=811, version=34, score=9.0720825, payload={'document': '  We construct the most general non-extremal deformation of the D-instanton solution with maximal rotational symmetry. The general non-supersymmetric solution carries electric charges of the SL(2,R) symmetry, which correspond to each of the three conjugacy classes of SL(2,R). Our calculations naturally generalise to arbitrary dimensions and arbitrary dilaton couplings.   We show that for specific values of the dilaton coupling parameter, the non-extremal instanton solutions can be viewed as wormholes of non-extremal Reissner-Nordstr\\\\\"om black holes in one higher dimension. We extend this result by showing that for other values of the dilaton coupling parameter, the non-extremal instanton solutions can be uplifted to non-extremal non-dilatonic p-branes in p+1 dimensions higher.   Finally, we attempt to consider the solutions as instantons of (compactified) type IIB superstring theory. In particular, we derive an elegant formula for the instanton action. We conjecture that the non-extremal D-instantons can contribute to the R^8-terms in the type IIB string effective action. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=87, version=5, score=8.645179, payload={'document': '  We have analyzed the 241 bursts for which peak counts $\\\\C$ exist in the publicly available Burst and Transient Source Experiment (BATSE) catalog. Introducing peak counts in 1024 ms as a measure of burst brightness $\\\\B$ and the ratio of peak counts in 64 and 1024 ms as a measure of short timescale variability $\\\\V$, we find a statistically significant correlation between the brightness and the short timescale variability of \\\\g-ray bursts. The bursts which are smoother on short timescales are both faint and bright, while the bursts which are variable on short timescales are faint only, suggesting the existence of two distinct morphological classes of bursts. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=723, version=30, score=8.241393, payload={'document': \"  Novelty attracts attention like popularity. Hence predicting novelty is as important as popularity. Novelty is the side effect of competition and aging in evolving systems. Recent behavior or recent link gain in networks plays an important role in emergence or trend. We exploited this wisdom and came up with two models considering different scenarios and systems. Where recent behavior dominates over total behavior (total link gain) in the first one, and recent behavior is as important as total behavior for future link gain in second one. It suppose that random walker walks on a network and can jump to any node, the probablity of jumping or making connection to other node is based on which node is recently more active or receiving more links. In our assumption random walker can also jump to node which is already popular but recently not popular. We are able to predict rising novelties or popular nodes which is generally suppressed under preferential attachment effect. To show performance of our model we have conducted experiments on four real data sets namely, MovieLens, Netflix, Facebook and Arxiv High Energy Physics paper citation. For testing our model we used four information retrieval indices namely Precision, Novelty, Area Under Receiving Operating Characteristic(AUC) and Kendal's rank correlation coefficient. We have used four benchmark models for validating our proposed models. Although our model doesn't perform better in all the cases but, it has theoretical significance in working better for recent behavior dominant systems. \", 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=665, version=28, score=8.123102, payload={'document': '  Exploring the Collatz Conjecture and changing the expression from 3n + 1 to 5n + 1, we found patterns in different sets of numbers. Some numbers reduce to one (as stated in the Collatz Conjecture), some might escape to infinity, and some get stuck in repeating cycles. To further explore the patterns involved in Collatz-like expressions, we changed the expression to 3n+5 and found connections between 3n+1 and 3n+5. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=624, version=26, score=7.7410836, payload={'document': \"  The glitch activity of young pulsars arises from the exchange of angular momentum between the crust and the interior of the star. Recently, it was inferred that the moment of inertia of the crust of a neutron star is not sufficient to explain the observed glitches. Such estimates are presumed in the Einstein's general relativity in describing the hydrostatic equilibrium of neutron stars. The crust of the neutron star has a space-time curvature of 14 orders of magnitude larger than that probed in solar system tests. This makes gravity the weakest constrained physics input in the crust related processes. We calculate the ratio of the crustal to the total moment of inertia of neutron stars in the scalar-tensor theory of gravity and the non-perturbative $f({\\\\cal R})={\\\\cal R}+ a {\\\\cal R}^2$ gravity. We find for the former that the crust to core ratio of the moment of inertia does not change significantly from what is inferred in general relativity. For the latter we find that the ratio increases significantly from what is inferred in general relativity in the case of high mass objects. Our results suggest that the glitch activity of pulsars may be used to probe gravity models, although the gravity models explored in this work are not appropriate candidates. \", 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=150, version=8, score=7.602549, payload={'document': '  We present the results of a study of the photospheric abundances of the HgMn star HD 175640, conducted using archival ESO-UVES spectra. A large number of unblended (titanium, chromium, manganese and iron) lines were studied to search for the presence of chemical stratification in the atmosphere of this star. The selected lines are located in the visible region of the spectrum, longward of the Balmer jump, in orders with S/N $\\\\geq$ 300. We derived the abundance of each element by calculating independently the abundance associated each line. We then characterized the depth of formation of each line, and examined the dependence of abundance on optical depth. Titanium, chromium, manganese and iron show no variation of their abundance with optical depth. These four elements do not appear to be strongly stratified in the atmosphere of HD 175640. This indicates that if stratification occurs, it must be in atmospheric layers which are not diagnosed by the spectral lines studied, or that it is too weak to detect using these data. We also report evidence that HD 175640 is an SB1, and furthermore report anomalous shifts of some strong Fe {\\\\sc II} lines, the origin of which is unclear. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=953, version=40, score=7.4195275, payload={'document': '  In this paper we are going to review the latest estimates for the particle background expected on the X-IFU instrument onboard of the ATHENA mission. The particle background is induced by two different particle populations: the so called \"soft protons\" and the Cosmic rays. The first component is composed of low energy particles (< 100s keV) that get funnelled by the mirrors towards the focal plane, losing part of their energy inside the filters and inducing background counts inside the instrument sensitivity band. The latter component is induced by high energy particles (> 100 MeV) that possess enough energy to cross the spacecraft and reach the detector from any direction, depositing a small fraction of their energy inside the instrument. Both these components are estimated using Monte Carlo simulations and the latest results are presented here. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=562, version=24, score=7.189059, payload={'document': '  Noiseless optical components are critical for applications ranging from metrology to quantum communication. Here we characterize several commercial telecom C-band fiber components for parasitic noise using a tunable laser. We observe the spectral signature of trace concentrations of erbium in all devices from the underlying optical crystals including YVO4, LiNbO3, TeO2 and AMTIR glass. Due to the long erbium lifetime, these signals are challenging to mitigate at the single photon level in the telecom range, and suggests the need for higher purity optical crystals. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=649, version=27, score=7.1804676, payload={'document': \"  The standard recurrent neural network language model (RNNLM) generates sentences one word at a time and does not work from an explicit global sentence representation. In this work, we introduce and study an RNN-based variational autoencoder generative model that incorporates distributed latent representations of entire sentences. This factorization allows it to explicitly model holistic properties of sentences such as style, topic, and high-level syntactic features. Samples from the prior over these sentence representations remarkably produce diverse and well-formed sentences through simple deterministic decoding. By examining paths through this latent space, we are able to generate coherent novel sentences that interpolate between known sentences. We present techniques for solving the difficult learning problem presented by this model, demonstrate its effectiveness in imputing missing words, explore many interesting properties of the model's latent sentence space, and present negative results on the use of the model in language modeling. \", 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=383, version=17, score=6.9955482, payload={'document': '  We construct the Killing superalgebra of supersymmetric backgrounds of ten-dimensional heterotic and type II supergravities and prove that it is a Lie superalgebra. We also show that if the fraction of supersymmetry preserved by the background is greater than 1/2, in the heterotic case, or greater than 3/4 in the type II case, then the background is locally homogeneous. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=372, version=16, score=6.96019, payload={'document': '  Reliable tools to extract patterns from high-dimensionality spaces are becoming more necessary as astronomical datasets increase both in volume and complexity. Contrastive Learning is a self-supervised machine learning algorithm that extracts informative measurements from multi-dimensional datasets, which has become increasingly popular in the computer vision and Machine Learning communities in recent years. To do so, it maximizes the agreement between the information extracted from augmented versions of the same input data, making the final representation invariant to the applied transformations. Contrastive Learning is particularly useful in astronomy for removing known instrumental effects and for performing supervised classifications and regressions with a limited amount of available labels, showing a promising avenue towards \\\\emph{Foundation Models}. This short review paper briefly summarizes the main concepts behind contrastive learning and reviews the first promising applications to astronomy. We include some practical recommendations on which applications are particularly attractive for contrastive learning. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=99, version=5, score=6.8230047, payload={'document': '  Having shown early promise, free-space optical communications (FSO) face formidable challenges in the age of information explosion. The ever-growing demand for greater channel communication capacity is one of the challenges. The inter-channel crosstalk, which severely degrades the quality of transmitted information, creates another roadblock in the way of efficient FSO implementation. Here we advance theoretically and realize experimentally a potentially high-capacity FSO protocol that enables high-fidelity transfer of an image, or set of images through a complex environment. In our protocol, we complement random light structuring at the transmitter with a deep learning image classification platform at the receiver. Multiplexing novel, independent, mutually orthogonal degrees of freedom available to structured random light can potentially significantly boost the channel communication capacity of our protocol without introducing any deleterious crosstalk. Specifically, we show how one can multiplex the degrees of freedom associated with the source coherence radius and a spatial position of a beamlet within an array of structured random beams to greatly enhance the capacity of our communication link. The superb resilience of structured random light to environmental noise, as well as extreme efficiency of deep learning networks at classifying images guarantees high-fidelity image transfer within the framework of our protocol. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=98, version=5, score=6.796772, payload={'document': '  Using exact solutions of the three-body problem, the spectrum of isolated three-body resonances for two identical particles interacting with another one is derived in the regime of large scattering length. The universality of the problem is depicted by using a contact model parameterized by two three-body parameters and the scattering length. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=308, version=14, score=6.3630013, payload={'document': '  Current top-performing blind perceptual image quality prediction models are generally trained on legacy databases of human quality opinion scores on synthetically distorted images. Therefore they learn image features that effectively predict human visual quality judgments of inauthentic, and usually isolated (single) distortions. However, real-world images usually contain complex, composite mixtures of multiple distortions. We study the perceptually relevant natural scene statistics of such authentically distorted images, in different color spaces and transform domains. We propose a bag of feature-maps approach which avoids assumptions about the type of distortion(s) contained in an image, focusing instead on capturing consistencies, or departures therefrom, of the statistics of real world images. Using a large database of authentically distorted images, human opinions of them, and bags of features computed on them, we train a regressor to conduct image quality prediction. We demonstrate the competence of the features towards improving automatic perceptual quality prediction by testing a learned algorithm using them on a benchmark legacy database as well as on a newly introduced distortion-realistic resource called the LIVE In the Wild Image Quality Challenge Database. We extensively evaluate the perceptual quality prediction model and algorithm and show that it is able to achieve good quality prediction power that is better than other leading models. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=676, version=29, score=6.320277, payload={'document': '  Mutation testing is vital for ensuring software quality. However, the presence of equivalent mutants is known to introduce redundant cost and bias issues, hindering the effectiveness of mutation testing in practical use. Although numerous equivalent mutant detection (EMD) techniques have been proposed, they exhibit limitations due to the scarcity of training data and challenges in generalizing to unseen mutants. Recently, large language models (LLMs) have been extensively adopted in various code-related tasks and have shown superior performance by more accurately capturing program semantics. Yet the performance of LLMs in equivalent mutant detection remains largely unclear. In this paper, we conduct an empirical study on 3,302 method-level Java mutant pairs to comprehensively investigate the effectiveness and efficiency of LLMs for equivalent mutant detection. Specifically, we assess the performance of LLMs compared to existing EMD techniques, examine the various strategies of LLMs, evaluate the orthogonality between EMD techniques, and measure the time overhead of training and inference. Our findings demonstrate that LLM-based techniques significantly outperform existing techniques (i.e., the average improvement of 35.69% in terms of F1-score), with the fine-tuned code embedding strategy being the most effective. Moreover, LLM-based techniques offer an excellent balance between cost (relatively low training and inference time) and effectiveness. Based on our findings, we further discuss the impact of model size and embedding quality, and provide several promising directions for future research. This work is the first to examine LLMs in equivalent mutant detection, affirming their effectiveness and efficiency. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=171, version=8, score=6.1290665, payload={'document': '  A spin model (for link invariants) is a square matrix $W$ which satisfies certain axioms. For a spin model $W$, it is known that $W^TW^{-1}$ is a permutation matrix, and its order is called the index of $W$. F. Jaeger and K. Nomura found spin models of index 2, by modifying the construction of symmetric spin models from Hadamard matrices. The aim of this paper is to give a construction of spin models of an arbitrary even index from any Hadamard matrix. In particular, we show that our spin models of indices a power of 2 are new. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=871, version=36, score=6.095655, payload={'document': '  In contrary to authors of Phys. Rev. Lett. 93, 126406 (2004) claiming \"the band picture to be a reasonable starting point for the description of the electronic structure of NiO, much better than the ligand-field picture\", we argue that the many-electron CEF approach is physically adequate starting point for discussion of the electronic structure and magnetism of NiO. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=840, version=35, score=6.0911026, payload={'document': '  Here we describe the form of the Asymmetric Superfluid Local Density Approximation (ASLDA), a Density Functional Theory (DFT) used to model the two-component unitary Fermi gas. We give the rational behind the functional, and describe explicitly how we determine the form of the DFT from the to the available numerical and experimental data. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=770, version=32, score=6.0500674, payload={'document': '  Vivid structural colors in birds are a conspicuous and vital part of their phenotype. They are produced by a rich diversity of integumentary photonic nanostructures in skin and feathers. Unlike pigmentary coloration, whose molecular genetic basis is being elucidated, little is known regarding the pathways underpinning organismal structural coloration. Here, we review available data on the development of avian structural colors. In particular, feather photonic nanostructures are understood to be intracellularly self-assembled by physicochemical forces typically seen in soft colloidal systems. We identify promising avenues for future research that can address current knowledge gaps, which is also highly relevant for the sustainable engineering of advanced bioinspired and biomimetic materials. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=369, version=16, score=5.98874, payload={'document': \"  While traditional corpus-level evaluation metrics for machine translation (MT) correlate well with fluency, they struggle to reflect adequacy. Model-based MT metrics trained on segment-level human judgments have emerged as an attractive replacement due to strong correlation results. These models, however, require potentially expensive re-training for new domains and languages. Furthermore, their decisions are inherently non-transparent and appear to reflect unwelcome biases. We explore the simple type-based classifier metric, MacroF1, and study its applicability to MT evaluation. We find that MacroF1 is competitive on direct assessment, and outperforms others in indicating downstream cross-lingual information retrieval task performance. Further, we show that MacroF1 can be used to effectively compare supervised and unsupervised neural machine translation, and reveal significant qualitative differences in the methods' outputs. \", 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=745, version=31, score=5.9479227, payload={'document': \"  In this article, we consider the radial Dunkl geometric case $k=1$ corresponding to flat Riemannian symmetric spaces in the complex case and we prove exact estimates for the positive valued Dunkl kernel and for the radial heat kernel.   Dans cet article, nous consid\\\\'erons le cas g\\\\'eom\\\\'etrique radial de Dunkl $ k = 1 $ correspondant aux espaces sym\\\\'etriques riemanniens plats dans le cas complexe et nous prouvons des estimations exactes pour le noyau de Dunkl \\\\`a valeur positive et pour le noyau de chaleur radial \", 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=111, version=6, score=5.847401, payload={'document': '  Correlation filters take advantage of specific properties in the Fourier domain allowing them to be estimated efficiently: O(NDlogD) in the frequency domain, versus O(D^3 + ND^2) spatially where D is signal length, and N is the number of signals. Recent extensions to correlation filters, such as MOSSE, have reignited interest of their use in the vision community due to their robustness and attractive computational properties. In this paper we demonstrate, however, that this computational efficiency comes at a cost. Specifically, we demonstrate that only 1/D proportion of shifted examples are unaffected by boundary effects which has a dramatic effect on detection/tracking performance. In this paper, we propose a novel approach to correlation filter estimation that: (i) takes advantage of inherent computational redundancies in the frequency domain, and (ii) dramatically reduces boundary effects. Impressive object tracking and detection results are presented in terms of both accuracy and computational efficiency. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=54, version=4, score=5.766935, payload={'document': '  We elaborate on the proposed general boundary formulation as an extension of standard quantum mechanics to arbitrary (or no) backgrounds. Temporal transition amplitudes are generalized to amplitudes for arbitrary spacetime regions. State spaces are associated to general (not necessarily spacelike) hypersurfaces. We give a detailed foundational exposition of this approach, including its probability interpretation and a list of core axioms. We explain how standard quantum mechanics arises as a special case. We include a discussion of probability conservation and unitarity, showing how these concepts are generalized in the present framework. We formulate vacuum axioms and incorporate spacetime symmetries into the framework. We show how the Schroedinger-Feynman approach is a suitable starting point for casting quantum field theories into the general boundary form. We discuss the role of operators. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=958, version=40, score=5.66364, payload={'document': '  The inclusive jet differential cross section has been measured for jet transverse energies, $E_T$, from 15 to 440 GeV, in the pseudorapidity region 0.1$\\\\leq | \\\\eta| \\\\leq $0.7. The results are based on 19.5 pb$^{-1}$ of data collected by the CDF collaboration at the Fermilab Tevatron collider. The data are compared with QCD predictions for various sets of parton distribution functions. The cross section for jets with $E_T>200$ GeV is significantly higher than current predictions based on O($\\\\alpha_s^3$) perturbative QCD calculations. Various possible explanations for the high-$E_T$ excess are discussed. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=81, version=5, score=5.629813, payload={'document': '  We derive an analytic expression for the temperature dependent critical magnetic field parallel to ultrathin superconducting films with Rashba spin-orbit interaction. Thereby we cover the range from small to large spin-orbit interactions $\\\\lambda$ compared with the gap parameter $\\\\Delta_0$. We find that at a critical spin-orbit energy $\\\\lambda_c$ a first-order phase transition takes place at which the pairing momentum of the Cooper pairs changes discontinuously. We speculate that this might give raise to new phenomena. With increasing $\\\\lambda/\\\\Delta_0$, the pair formation changes from interband to intraband pairing. For $\\\\lambda>\\\\lambda_c$, a dimensional cross-over of the critical field from two to one dimension is taking place. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=459, version=20, score=5.5702224, payload={'document': '  We show that a germ of a real analytic Lorentz metric on ${\\\\bf R}^3$ which is locally homogeneous on an open set containing the origin in its closure is necessarily locally homogeneous. We classifiy Lie algebras that can act quasihomogeneously---meaning they act transitively on an open set admitting the origin in its closure, but not at the origin---and isometrically for such a metric. In the case that the isotropy at the origin of a quasihomogeneous action is semisimple, we provide a complete set of normal forms of the metric and the action. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=540, version=23, score=5.5497828, payload={'document': '  We study the effect of periodic boundary conditions on chiral symmetry breaking and its restoration in Quantum Chromodynamics. As an effective model of the effective potential for the quark condensate, we use the quark-meson model, while the theory is quantized in a cubic box of size $L$. After specifying a renormalization prescription for the vacuum quark loop, we study the condensate at finite temperature, $T$, and quark chemical potential, $\\\\mu$. We find that lowering $L$ leads to a catalysis of chiral symmetry breaking. The excitation of the zero mode leads to a jump in the condensate at low temperature and high density, that we suggest to interpret as a gas-liquid phase transition that takes place between the chiral symmetry broken phase (hadron gas) and chiral symmetry restored phase (quark matter). We characterize this intermediate phase in terms of the increase of the baryon density, and of the correlation length of the fluctuations of the order parameter: for small enough $L$ the correlation domains occupy a substantial portion of the volume of the system, and the fluctuations are comparable to those in the critical region. For these reasons, we dub this phase as the {\\\\it subcritical liquid}. The qualitative picture that we draw is in agreement with previous studies based on similar effective models. We also clarify the discrepancy on the behavior of the critical temperature versus $L$ found in different models. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=391, version=17, score=5.210629, payload={'document': '  Specialist language models (LMs) focus on a specific task or domain on which they often outperform generalist LMs of the same size. However, the specialist data needed to pretrain these models is only available in limited amount for most tasks. In this work, we build specialist models from large generalist training sets instead. We adjust the training distribution of the generalist data with guidance from the limited domain-specific data. We explore several approaches, with clustered importance sampling standing out. This method clusters the generalist dataset and samples from these clusters based on their frequencies in the smaller specialist dataset. It is scalable, suitable for pretraining and continued pretraining, it works well in multi-task settings. Our findings demonstrate improvements across different domains in terms of language modeling perplexity and accuracy on multiple-choice question tasks. We also present ablation studies that examine the impact of dataset sizes, clustering configurations, and model sizes. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=815, version=34, score=5.2035046, payload={'document': '  By consolidating scattered knowledge, the literature review provides a comprehensive understanding of the investigated topic. However, reading, conducting, or peer-reviewing review papers generally demands a significant investment of time and effort from researchers. To improve efficiency, this paper aims to provide a thorough review of reviews in the PAMI field from diverse perspectives. First, this paper proposes several article-level, field-normalized, and large language model-empowered bibliometric indicators to evaluate reviews. To facilitate this, a meta-data database dubbed RiPAMI, and a topic dataset are constructed. Second, based on these indicators, the study presents comparative analyses of representative reviews, unveiling the characteristics of publications across various fields, periods, and journals. The newly emerging AI-generated literature reviews are also appraised, and the observed differences suggest that most AI-generated reviews still lag behind human-authored reviews in multiple aspects. Third, we briefly provide a subjective evaluation of representative PAMI reviews and introduce a paper structure-based typology of literature reviews. This typology may improve the clarity and effectiveness for scholars in reading and writing reviews, while also serving as a guide for AI systems in generating well-organized reviews. Finally, this work offers insights into the current challenges of literature reviews and envisions future directions for their development. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=838, version=35, score=5.0861497, payload={'document': '  In this article we investigate whether a theory based on a classical Lagrangian for the minimal Standard-Model Extension (SME) can be quantized such that the result is equal to the corresponding low-energy Hamilton operator obtained from the field-theory description. This analysis is carried out for the whole collection of minimal Lagrangians found in the literature. The upshot is that first quantization can be performed consistently. The unexpected observation is made that at first order in Lorentz violation and at second order in the velocity the Lagrangians are related to the Hamilton functions by a simple transformation. Under mild assumptions, it is shown that this holds universally. This result is used successfully to obtain classical Lagrangians for two complicated sectors of the minimal SME that have not been considered in the literature so far. Therefore, it will not be an obstacle anymore to derive such Lagrangians even for involved sets of coefficients - at least to the level of approximation stated above. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=766, version=32, score=5.0743785, payload={'document': '  In this paper, we develop a new rainbow Hamilton framework, which is of independent interest, settling the problem proposed by Gupta, Hamann, M\\\\\"{u}yesser, Parczyk, and Sgueglia when $k=3$, and draw the general conclusion for any $k\\\\geq3$ as follows. A $k$-graph system $\\\\textbf{H}=\\\\{H_i\\\\}_{i\\\\in[n]}$ is a family of not necessarily distinct $k$-graphs on the same $n$-vertex set $V$, moreover, a $k$-graph $H$ on $V$ is rainbow if $E(H)\\\\subseteq \\\\bigcup_{i\\\\in[n]}E(H_i)$ and $|E(H)\\\\cap E(H_i)|\\\\leq1$ for $i\\\\in[n]$. We show that given $\\\\gamma> 0$, sufficiently large $n$ and an $n$-vertex $k$-graph system $\\\\textbf{H}=\\\\{H_i\\\\}_{i\\\\in[n]}$ , if $\\\\delta_{k-2}(H_i)\\\\geq(5/9+\\\\gamma)\\\\binom{n}{2}$ for $i\\\\in[n]$ where $k\\\\geq3$, then there exists a rainbow tight Hamilton cycle. This result implies the conclusion in a single graph, which was proved by Lang and Sanhueza-Matamala [$J. Lond. Math. Soc., 2022$], Polcyn, Reiher, R\\\\\"{o}dl and Sch\\\\\"{u}lke [$J. Combin. Theory \\\\ Ser. B, 2021$] independently. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=453, version=20, score=5.047434, payload={'document': '  This posting addresses the incorrect information (about my papers) and analysis provided in the following articles: arXiv:1405.4000/Nanotechnology 26, 245202 (2015), arXiv:1412.7765/Scientific Reports 4, 7553 (2014), Appl. Phys. Lett. 105, 176101 (2014), arXiv:1407.2140/Appl. Phys. Lett. 105, 072408 (2014), arXiv:1402.5356/Appl. Phys. Lett. 104, 232403 (2014), Appl. Phys. Lett. 103, 232401 (2013), arXiv:1005.5358/Appl. Phys. Lett. 97, 173105 (2010) ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=921, version=38, score=4.8922024, payload={'document': '  Large language models (LLMs) can perform a wide range of tasks by following natural language instructions, without the necessity of task-specific fine-tuning. Unfortunately, the performance of LLMs is greatly influenced by the quality of these instructions, and manually writing effective instructions for each task is a laborious and subjective process. In this paper, we introduce Auto-Instruct, a novel method to automatically improve the quality of instructions provided to LLMs. Our method leverages the inherent generative ability of LLMs to produce diverse candidate instructions for a given task, and then ranks them using a scoring model trained on a variety of 575 existing NLP tasks. In experiments on 118 out-of-domain tasks, Auto-Instruct surpasses both human-written instructions and existing baselines of LLM-generated instructions. Furthermore, our method exhibits notable generalizability even with other LLMs that are not incorporated into its training process. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=564, version=24, score=4.831299, payload={'document': '  Our vision paper outlines a plan to improve the future of semantic interoperability in data spaces through the application of machine learning. The use of data spaces, where data is exchanged among members in a self-regulated environment, is becoming increasingly popular. However, the current manual practices of managing metadata and vocabularies in these spaces are time-consuming, prone to errors, and may not meet the needs of all stakeholders. By leveraging the power of machine learning, we believe that semantic interoperability in data spaces can be significantly improved. This involves automatically generating and updating metadata, which results in a more flexible vocabulary that can accommodate the diverse terminologies used by different sub-communities. Our vision for the future of data spaces addresses the limitations of conventional data exchange and makes data more accessible and valuable for all members of the community. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=685, version=29, score=4.771852, payload={'document': \"  We rediscuss recent derivations of kinetic equations based on the Kaniadakis' entropy concept. Our primary objective here is to derive a kinetical version of the second law of thermodynamycs in such a $\\\\kappa$-framework. To this end, we assume a slight modification of the molecular chaos hypothesis. For the $H_{\\\\kappa}$-theorem, it is shown that the collisional equilibrium states (null entropy source term) are described by a $\\\\kappa$-power law extension of the exponential distribution and, as should be expected, all these results reduce to the standard one in the limit $\\\\kappa\\\\to 0$. \", 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=986, version=41, score=4.764631, payload={'document': '  In an Online Social Network (OSN), users can create a unique public persona by crafting a user identity that may encompass profile details, content, and network-related information. As a result, a relevant task of interest is related to the ability to link identities across different OSNs. Linking users across social networks can have multiple implications in several contexts both at the individual level and at the group level. At the individual level, the main interest in linking the same identity across social networks is to enable a better knowledge of each user. At the group level, linking user identities through different OSNs helps in predicting user behaviors, network dynamics, information diffusion, and migration phenomena across social media. The process of tying together user accounts on different OSNs is challenging and has attracted more and more research attention in the last fifteen years. The purpose of this work is to provide a comprehensive review of recent studies (from 2016 to the present) on User Identity Linkage (UIL) methods across online social networks. This review aims to offer guidance for other researchers in the field by outlining the main problem formulations, the different feature extraction strategies, algorithms, machine learning models, datasets, and evaluation metrics proposed by researchers working in this area. The proposed overview takes a pragmatic perspective to highlight the concrete possibilities for accomplishing this task depending on the type of available data. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=885, version=37, score=4.7330675, payload={'document': '  Recent successes in autoregressive (AR) generation models, such as the GPT series in natural language processing, have motivated efforts to replicate this success in visual tasks. Some works attempt to extend this approach to autonomous driving by building video-based world models capable of generating realistic future video sequences and predicting ego states. However, prior works tend to produce unsatisfactory results, as the classic GPT framework is designed to handle 1D contextual information, such as text, and lacks the inherent ability to model the spatial and temporal dynamics essential for video generation. In this paper, we present DrivingWorld, a GPT-style world model for autonomous driving, featuring several spatial-temporal fusion mechanisms. This design enables effective modeling of both spatial and temporal dynamics, facilitating high-fidelity, long-duration video generation. Specifically, we propose a next-state prediction strategy to model temporal coherence between consecutive frames and apply a next-token prediction strategy to capture spatial information within each frame. To further enhance generalization ability, we propose a novel masking strategy and reweighting strategy for token prediction to mitigate long-term drifting issues and enable precise control. Our work demonstrates the ability to produce high-fidelity and consistent video clips of over 40 seconds in duration, which is over 2 times longer than state-of-the-art driving world models. Experiments show that, in contrast to prior works, our method achieves superior visual quality and significantly more accurate controllable future video generation. Our code is available at https://github.com/YvanYin/DrivingWorld. ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None), ScoredPoint(id=797, version=33, score=4.7139316, payload={'document': '  Assuming that the order parameter corresponds to an equal spin triplet pairing state, we calculate the angular dependence of the upper critical field in triplet quasi-one-dimensional superconductors at high magnetic fields applied alongthe yz plane ', 'user_id': 'user_3'}, vector=None, shard_key=None, order_value=None)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run final query\n",
    "results = client.query_points(\n",
    "    COLLECTION_NAME,\n",
    "    prefetch=prefetch,\n",
    "    query=late_vectors,\n",
    "    using=\"colbert\",\n",
    "    with_payload=True,\n",
    "    limit=50,\n",
    "    query_filter=user_filter\n",
    ")\n",
    "\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb73c33",
   "metadata": {},
   "source": [
    "### Display Results\n",
    "\n",
    "To better analyze our results, I've cleaned them up here into a pretty format from Polars."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "41edf645",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (50, 4)\n",
      "┌─────┬───────────┬─────────┬──────────────────────────────────────────────────────────────────────┐\n",
      "│ id  ┆ score     ┆ user_id ┆ payload                                                              │\n",
      "│ --- ┆ ---       ┆ ---     ┆ ---                                                                  │\n",
      "│ i64 ┆ f64       ┆ str     ┆ str                                                                  │\n",
      "╞═════╪═══════════╪═════════╪══════════════════════════════════════════════════════════════════════╡\n",
      "│ 702 ┆ 16.326674 ┆ user_3  ┆ High redshift galaxy clusters have traditionally been a fruitful     │\n",
      "│     ┆           ┆         ┆ place to study galaxy evolution. I review various search strategies  │\n",
      "│     ┆           ┆         ┆ for finding clusters at z > 1. Most efforts to date have concentr…   │\n",
      "│ 726 ┆ 14.451191 ┆ user_3  ┆ Using a subsample of 79 nearby clusters from the RASS-SDSS galaxy    │\n",
      "│     ┆           ┆         ┆ cluster catalogue of Popesso et al. (2005a), we perform a regression │\n",
      "│     ┆           ┆         ┆ analysis between the cluster integrated star formation rate (Si…     │\n",
      "│ 632 ┆ 13.75998  ┆ user_3  ┆ We present the detection of compact radio structures of fourteen     │\n",
      "│     ┆           ┆         ┆ radio-loud narrow line Seyfert 1 (NLS1) galaxies from Very Long      │\n",
      "│     ┆           ┆         ┆ Baseline Array observations at 5 GHz, which were performed in 2013.  │\n",
      "│     ┆           ┆         ┆ W…                                                                   │\n",
      "│ 290 ┆ 12.638304 ┆ user_3  ┆ We identify 228 periodic variables in the region of young open       │\n",
      "│     ┆           ┆         ┆ cluster NGC 281 using time series photometry carried out from 1 m    │\n",
      "│     ┆           ┆         ┆ class ARIES telescopes, Nainital. The cluster membership of these    │\n",
      "│     ┆           ┆         ┆ ide…                                                                 │\n",
      "│ 114 ┆ 11.706559 ┆ user_3  ┆ The IceCube Neutrino Observatory is the world's largest high energy  │\n",
      "│     ┆           ┆         ┆ neutrino telescope, using the Antarctic ice cap as a Cherenkov       │\n",
      "│     ┆           ┆         ┆ detector medium. DeepCore, the low energy extension to IceCube, is … │\n",
      "│ …   ┆ …         ┆ …       ┆ …                                                                    │\n",
      "│ 564 ┆ 4.831299  ┆ user_3  ┆ Our vision paper outlines a plan to improve the future of semantic   │\n",
      "│     ┆           ┆         ┆ interoperability in data spaces through the application of machine   │\n",
      "│     ┆           ┆         ┆ learning. The use of data spaces, where data is exchanged among …    │\n",
      "│ 685 ┆ 4.771852  ┆ user_3  ┆ We rediscuss recent derivations of kinetic equations based on the    │\n",
      "│     ┆           ┆         ┆ Kaniadakis' entropy concept. Our primary objective here is to derive │\n",
      "│     ┆           ┆         ┆ a kinetical version of the second law of thermodynamycs in such…     │\n",
      "│ 986 ┆ 4.764631  ┆ user_3  ┆ In an Online Social Network (OSN), users can create a unique public  │\n",
      "│     ┆           ┆         ┆ persona by crafting a user identity that may encompass profile       │\n",
      "│     ┆           ┆         ┆ details, content, and network-related information. As a result, a r… │\n",
      "│ 885 ┆ 4.7330675 ┆ user_3  ┆ Recent successes in autoregressive (AR) generation models, such as   │\n",
      "│     ┆           ┆         ┆ the GPT series in natural language processing, have motivated        │\n",
      "│     ┆           ┆         ┆ efforts to replicate this success in visual tasks. Some works        │\n",
      "│     ┆           ┆         ┆ attempt…                                                             │\n",
      "│ 797 ┆ 4.7139316 ┆ user_3  ┆ Assuming that the order parameter corresponds to an equal spin       │\n",
      "│     ┆           ┆         ┆ triplet pairing state, we calculate the angular dependence of the    │\n",
      "│     ┆           ┆         ┆ upper critical field in triplet quasi-one-dimensional                │\n",
      "│     ┆           ┆         ┆ superconductors…                                                     │\n",
      "└─────┴───────────┴─────────┴──────────────────────────────────────────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "output = []\n",
    "for point in results.points:\n",
    "    output.append({\n",
    "        'id': point.id,\n",
    "        'score': point.score,\n",
    "        'user_id': point.payload.get('user_id', ''),\n",
    "        'payload': point.payload.get('document', '')\n",
    "    })\n",
    "\n",
    "df = pl.DataFrame(output)\n",
    "pl.Config.set_fmt_str_lengths(200) # Show up to 200 characters from abstract\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fbd4ac",
   "metadata": {},
   "source": [
    "## Conclusion and Takeaways\n",
    "\n",
    "### What We've Accomplished\n",
    "\n",
    "**Multi-Modal Embeddings**  \n",
    "- Dense embeddings for semantic understanding  \n",
    "- Sparse embeddings for keyword precision  \n",
    "- Late interaction embeddings for fine-grained relevance  \n",
    "\n",
    "**Production-Ready Architecture**  \n",
    "- Scalable vector database with Qdrant  \n",
    "- Efficient batch processing and indexing  \n",
    "- Multi-tenant security with user filtering  \n",
    "\n",
    "**Advanced Search Capabilities**  \n",
    "- Hybrid retrieval combining multiple signals  \n",
    "- Sophisticated ranking and reranking  \n",
    "- Flexible query processing pipeline  \n",
    "\n",
    "### Potential Enhancements\n",
    "\n",
    "- Implement query-time filtering for better performance  \n",
    "- Add caching for frequently accessed embeddings\n",
    "- Add GPU acceleration to embedding generation \n",
    "- Parallelize long-running processes for faster execution \n",
    "- Integrate with LLMs for a more conversation experience (RAG) \n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Hybrid approaches outperform single methods** by combining different strengths  \n",
    "2. **Late interaction models** provide exceptional precision for text search  \n",
    "3. **Vector databases** enable sophisticated multi-modal search at scale  \n",
    "4. **Security considerations** are crucial for multi-tenant applications  \n",
    "\n",
    "This hybrid search system provides a solid foundation for building production-grade search applications that deliver both high recall and precision across diverse query types.\n",
    "\n",
    "Thanks for the fun project!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
